{"/nishad/srake/about/":{"data":{"":"This is the about page."},"title":"About"},"/nishad/srake/docs/":{"data":{"":"Welcome to the srake documentation! This guide will help you get started with processing NCBI SRA metadata efficiently.","features#Features":"Performance: Efficient record processing Memory Management: Streaming architecture for large files Pipeline: HTTP → Gzip → Tar → XML → Database streaming Filtering: Filter by taxonomy, organism, platform, and more Resume Support: Recovery from interruptions Search: Full-text search with SQLite backend","key-features#Key Features":"Getting StartedInstall and run srake in minutes Filtering SystemProcess only the data you need Resume CapabilityHandle interruptions gracefully API ReferenceREST API and Go library","quick-example#Quick Example":"# Install srake go install github.com/nishad/srake/cmd/srake@latest # Ingest SRA metadata with filters srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --platforms ILLUMINA \\ --strategies RNA-Seq # Search the database srake search \"homo sapiens\" --limit 10 # Start API server srake server --port 8080","what-is-srake#What is srake?":"srake is a tool for processing and querying NCBI SRA (Sequence Read Archive) metadata. Built with a streaming architecture, srake can process large compressed archives without intermediate storage."},"title":"Docs"},"/nishad/srake/docs/api/":{"data":{"authentication#Authentication":"The API currently does not require authentication for read operations.","base-url#Base URL":"http://localhost:8080/api","client-libraries#Client Libraries":"","conversion-endpoints#Conversion Endpoints":"","cors#CORS":"CORS headers are enabled by default in development mode. For production, configure appropriate CORS policies based on your deployment needs.","curlbash#curl/bash":"#!/bin/bash # Search and save results curl -s \"http://localhost:8080/api/search?q=cancer\u0026format=json\" \\ | jq '.hits[].fields.run_accession' \\ \u003e accessions.txt # Batch download using results while read -r acc; do echo \"Processing $acc\" srake download \"$acc\" done \u003c accessions.txt","endpoints#Endpoints":"","error-handling#Error Handling":"All endpoints return appropriate HTTP status codes:\n200 OK: Successful request 400 Bad Request: Invalid parameters 404 Not Found: Resource not found 500 Internal Server Error: Server error Error responses include a JSON body:\n{ \"error\": \"Invalid query syntax\", \"message\": \"Unmatched quotes in query string\", \"code\": \"INVALID_QUERY\" }","from-pysradb#From pysradb":"# pysradb from pysradb import SRAdb db = SRAdb() df = db.search(\"cancer\", detailed=True) # srake API equivalent import pandas as pd import requests response = requests.get('http://localhost:8080/api/search', params={'q': 'cancer', 'format': 'json'}) df = pd.DataFrame([hit['fields'] for hit in response.json()['hits']])","from-sradb-r#From SRAdb (R)":"# SRAdb library(SRAdb) sqlfile \u003c- getSRAdbFile() con \u003c- dbConnect(SQLite(), sqlfile) rs \u003c- dbGetQuery(con, \"SELECT * FROM sra WHERE organism = 'Homo sapiens'\") # srake API equivalent library(httr) library(jsonlite) response \u003c- GET(\"http://localhost:8080/api/search\", query = list(organism = \"homo sapiens\")) rs \u003c- fromJSON(content(response, \"text\"))$hits","get-apiconvertaccession#\u003ccode\u003eGET /api/convert/{accession}\u003c/code\u003e":"Convert between different accession types.\nPath Parameters:\naccession: Source accession Query Parameters:\nto: Target type (GSE, GSM, SRP, SRX, SRR, PRJNA, etc.) Example Request:\n# Convert SRA to GEO curl \"http://localhost:8080/api/convert/SRP123456?to=GSE\" # Convert GEO to SRA curl \"http://localhost:8080/api/convert/GSM123456?to=SRX\" Response Format:\n{ \"source\": \"SRP123456\", \"source_type\": \"SRP\", \"target\": \"GSE98765\", \"target_type\": \"GSE\", \"status\": \"success\" }","get-apiexperimentsaccession#\u003ccode\u003eGET /api/experiments/{accession}\u003c/code\u003e":"Get all experiments for a study.\n# Get experiments for a study curl \"http://localhost:8080/api/experiments/SRP123456\"","get-apimetadataaccession#\u003ccode\u003eGET /api/metadata/{accession}\u003c/code\u003e":"Get metadata for a specific SRA accession.\nPath Parameters:\naccession: SRA accession (SRP, SRX, SRR, SRS, etc.) Query Parameters:\nformat: Output format (json, yaml, xml) expand: Include related entities (true/false) Example Request:\n# Get run metadata curl \"http://localhost:8080/api/metadata/SRR123456\" # Get study with expanded relations curl \"http://localhost:8080/api/metadata/SRP123456?expand=true\" Response Format:\n{ \"accession\": \"SRR123456\", \"type\": \"run\", \"metadata\": { \"run_accession\": \"SRR123456\", \"experiment_accession\": \"SRX123456\", \"study_accession\": \"SRP123456\", \"sample_accession\": \"SRS123456\", \"spots\": 25000000, \"bases\": 2500000000, \"published_date\": \"2024-01-15\" }, \"relations\": { \"experiment\": {...}, \"study\": {...}, \"sample\": {...} } }","get-apirunsaccession#\u003ccode\u003eGET /api/runs/{accession}\u003c/code\u003e":"Get all runs for a study, experiment, or sample.\nExample Request:\n# Get runs for a study curl \"http://localhost:8080/api/runs/SRP123456\" # Get runs for an experiment curl \"http://localhost:8080/api/runs/SRX123456\"","get-apisamplesaccession#\u003ccode\u003eGET /api/samples/{accession}\u003c/code\u003e":"Get all samples for a study or experiment.\n# Get samples for a study curl \"http://localhost:8080/api/samples/SRP123456\"","get-apisearch#\u003ccode\u003eGET /api/search\u003c/code\u003e":"Perform full-text search with advanced filtering capabilities.\nQuery Parameters:\nParameter Type Description Default q string Search query (supports advanced syntax) - advanced boolean Enable advanced query parsing false limit integer Maximum results to return 100 offset integer Pagination offset 0 organism string Filter by organism - platform string Filter by platform - library_strategy string Filter by library strategy - library_source string Filter by library source - library_selection string Filter by library selection - library_layout string Filter by library layout - instrument string Filter by instrument model - date_from string Filter by date from (YYYY-MM-DD) - date_to string Filter by date to (YYYY-MM-DD) - spots_min integer Minimum number of spots - spots_max integer Maximum number of spots - bases_min integer Minimum number of bases - bases_max integer Maximum number of bases - format string Output format (json, csv, tsv, accession) json fields string Comma-separated list of fields to return all aggregate_by string Field to aggregate results by - count_only boolean Return only count false facets boolean Include facets in response false Example Request:\n# Basic search curl \"http://localhost:8080/api/search?q=homo+sapiens\u0026limit=10\" # Advanced search with filters curl \"http://localhost:8080/api/search?q=cancer\u0026organism=homo+sapiens\u0026platform=ILLUMINA\u0026library_strategy=RNA-Seq\" # Boolean query curl \"http://localhost:8080/api/search?q=organism:human+AND+library_strategy:RNA-Seq\u0026advanced=true\" # Aggregation curl \"http://localhost:8080/api/search?q=RNA-Seq\u0026aggregate_by=organism\" # Count only curl \"http://localhost:8080/api/search?q=cancer\u0026count_only=true\" Response Format (JSON):\n{ \"total\": 42156, \"limit\": 10, \"offset\": 0, \"hits\": [ { \"id\": \"SRR123456\", \"score\": 1.5432, \"fields\": { \"type\": \"run\", \"run_accession\": \"SRR123456\", \"experiment_accession\": \"SRX123456\", \"study_accession\": \"SRP123456\", \"organism\": \"Homo sapiens\", \"platform\": \"ILLUMINA\", \"library_strategy\": \"RNA-Seq\", \"spots\": 25000000, \"bases\": 2500000000 } } ], \"facets\": { \"organism\": { \"Homo sapiens\": 15234, \"Mus musculus\": 8921 }, \"platform\": { \"ILLUMINA\": 35678, \"OXFORD_NANOPORE\": 4521 } }, \"aggregations\": { \"organism\": [ {\"value\": \"Homo sapiens\", \"count\": 15234}, {\"value\": \"Mus musculus\", \"count\": 8921} ] } }","get-apistats#\u003ccode\u003eGET /api/stats\u003c/code\u003e":"Get database statistics.\nExample Request:\ncurl \"http://localhost:8080/api/stats\" Response Format:\n{ \"database\": { \"size\": 4567890123, \"path\": \"/data/SRAmetadb.sqlite\" }, \"tables\": { \"study\": 456789, \"experiment\": 2345678, \"sample\": 3456789, \"run\": 12345678 }, \"index\": { \"documents\": 18625590, \"size\": 1234567890, \"last_updated\": \"2024-01-15T10:30:00Z\" } }","get-apistudiesaccession#\u003ccode\u003eGET /api/studies/{accession}\u003c/code\u003e":"Get study information from any related accession.\n# Get study from run curl \"http://localhost:8080/api/studies/SRR123456\" # Get study from sample curl \"http://localhost:8080/api/studies/SRS123456\"","graphql-api#GraphQL API":"A GraphQL endpoint is under consideration for complex relationship queries.","javascriptnodejs#JavaScript/Node.js":"// Using fetch API const params = new URLSearchParams({ q: 'cancer', organism: 'homo sapiens', platform: 'ILLUMINA' }); fetch(`http://localhost:8080/api/search?${params}`) .then(res =\u003e res.json()) .then(data =\u003e { console.log(`Found ${data.total} results`); data.hits.forEach(hit =\u003e { console.log(hit.fields.run_accession); }); });","metadata-endpoints#Metadata Endpoints":"","migration-from-other-tools#Migration from Other Tools":"","performance-tips#Performance Tips":"Use specific filters: Narrow down results with filters to reduce response size Pagination: Use limit and offset for large result sets Field selection: Use fields parameter to return only needed data Aggregations: Use aggregate_by for analytics instead of fetching all data Caching: Implement client-side caching for frequently accessed data","post-apisearchindex#\u003ccode\u003ePOST /api/search/index\u003c/code\u003e":"Manage the search index.\nRequest Body:\n{ \"action\": \"build|rebuild|verify|stats\", \"batch_size\": 1000, \"workers\": 4 } Example Request:\n# Build index curl -X POST \"http://localhost:8080/api/search/index\" \\ -H \"Content-Type: application/json\" \\ -d '{\"action\": \"build\"}' # Get index stats curl -X POST \"http://localhost:8080/api/search/index\" \\ -H \"Content-Type: application/json\" \\ -d '{\"action\": \"stats\"}'","python#Python":"import requests # Search API response = requests.get('http://localhost:8080/api/search', params={ 'q': 'homo sapiens', 'platform': 'ILLUMINA', 'library_strategy': 'RNA-Seq', 'limit': 100 }) results = response.json() for hit in results['hits']: print(f\"{hit['fields']['run_accession']}: {hit['fields']['organism']}\")","r#R":"library(httr) library(jsonlite) # Search for RNA-Seq data response \u003c- GET(\"http://localhost:8080/api/search\", query = list( q = \"RNA-Seq\", organism = \"homo sapiens\", limit = 100 )) data \u003c- fromJSON(content(response, \"text\")) print(paste(\"Total results:\", data$total))","rate-limiting#Rate Limiting":"The API currently does not implement rate limiting. For production deployments, consider adding a reverse proxy with rate limiting capabilities.","relationship-endpoints#Relationship Endpoints":"","rest-api-reference#REST API Reference":"REST API ReferenceThe srake REST API provides programmatic access to all search and metadata functionality.","search-endpoints#Search Endpoints":"","starting-the-server#Starting the Server":"# Start with default settings srake server # Custom port and host srake server --port 8080 --host 0.0.0.0 # With custom database path srake server --db /path/to/SRAmetadb.sqlite # Development mode with verbose logging srake server --dev --log-level debug","statistics-endpoints#Statistics Endpoints":"","support#Support":"For API issues or feature requests, please visit:\nGitHub Issues API Documentation","webhooks#Webhooks":"Webhooks are not currently supported but are planned for future releases.","websocket-support#WebSocket Support":"Real-time updates via WebSocket are planned for future releases."},"title":"API Reference"},"/nishad/srake/docs/automation/":{"data":{"automation--scripting-guide#Automation \u0026amp; Scripting Guide":"Automation \u0026 Scripting Guidesrake is designed to work seamlessly in automated workflows and scripts, following clig.dev best practices for command-line interfaces.","batch-processing#Batch Processing":"# Process accessions from a file while IFS= read -r accession; do srake convert \"$accession\" --to GSE --quiet done \u003c accessions.txt # Or use stdin directly cat accessions.txt | srake convert --to GSE --output results.json","best-practices#Best Practices":"Always use --yes in scripts to avoid hanging on prompts Use --quiet to suppress non-essential output in production scripts Enable --debug when developing to understand command behavior Test with --dry-run first before running destructive operations Check exit codes for proper error handling Use structured output formats (JSON/TSV) for reliable parsing Leverage stdin for composability with other Unix tools Set appropriate timeouts for network operations in CI/CD","chaining-commands#Chaining Commands":"# Find all RNA-Seq experiments and download them srake search \"RNA-Seq\" --format tsv | \\ cut -f1 | \\ srake download --type fastq --parallel 4 # Convert a list of accessions cat accessions.txt | srake convert --to GSE --format json \u003e converted.json # Process search results through multiple tools srake search \"homo sapiens\" --limit 1000 | \\ grep \"ILLUMINA\" | \\ cut -f1 | \\ srake metadata --format json","cicd-integration#CI/CD Integration":"","cron-jobs#Cron Jobs":"Example cron job for automated daily ingestion:\n# Daily SRA metadata update at 2 AM 0 2 * * * /usr/local/bin/srake ingest --daily --yes --quiet \u003e\u003e /var/log/srake.log 2\u003e\u00261 # Weekly full ingest on Sundays 0 3 * * 0 /usr/local/bin/srake ingest --auto --yes --force \u003e\u003e /var/log/srake.log 2\u003e\u00261","debugging-scripts#Debugging Scripts":"Use the --debug flag to troubleshoot issues:\n# Enable debug output for detailed logging srake download SRR123456 --debug 2\u003e debug.log # Combine with verbose for maximum information srake convert SRP123456 --to GSE --debug --verbose","docker-integration#Docker Integration":"Run srake in containerized environments:\n# Non-interactive Docker execution docker run -v $(pwd)/data:/data \\ srake-image \\ srake ingest --auto --yes --db /data/metadata.db # Pipe data into containerized srake cat accessions.txt | docker run -i srake-image \\ srake convert --to GSE --format json","dry-run-mode#Dry Run Mode":"Test your commands without making changes using --dry-run:\n# Preview what would be downloaded srake download SRP123456 --dry-run # Check conversions before executing echo -e \"SRP001\\nSRP002\\nSRP003\" | srake convert --to GSE --dry-run","environment-variables#Environment Variables":"srake respects standard environment variables:\n# Disable colored output export NO_COLOR=1 # Custom database location export SRAKE_DB=/custom/path/metadata.db # Run with environment overrides NO_COLOR=1 srake search \"mouse\" --format table","error-handling#Error Handling":"srake follows Unix conventions for exit codes:\n0: Success 1: General error 2: Command line usage error #!/bin/bash set -e # Exit on any error # Check if download succeeded if srake download SRR123456 --yes --quiet; then echo \"Download successful\" else echo \"Download failed with exit code $?\" exit 1 fi","github-actions-example#GitHub Actions Example":"name: Update SRA Metadata on: schedule: - cron: '0 0 * * *' # Daily at midnight workflow_dispatch: jobs: update: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Setup Go uses: actions/setup-go@v4 with: go-version: '1.21' - name: Install srake run: go install github.com/nishad/srake/cmd/srake@latest - name: Update metadata run: | srake ingest --auto --yes --quiet srake db info - name: Upload database uses: actions/upload-artifact@v3 with: name: sra-metadata path: ./data/metadata.db","gitlab-ci-example#GitLab CI Example":"update-sra-metadata: stage: data script: - srake ingest --auto --yes --quiet - srake search \"homo sapiens\" --limit 100 --format json \u003e latest_human.json artifacts: paths: - ./data/metadata.db - latest_human.json only: - schedules","logging#Logging":"Redirect output streams for logging:\n# Log errors only srake ingest --auto 2\u003e errors.log # Log everything srake ingest --auto --verbose \u003e output.log 2\u003e\u00261 # Separate stdout and stderr srake search \"human\" \u003e results.txt 2\u003e errors.log # Tee for both console and file srake ingest --auto 2\u003e\u00261 | tee -a srake.log This guide ensures your srake automation is robust, maintainable, and follows Unix philosophy for maximum interoperability.","non-interactive-mode#Non-Interactive Mode":"When running srake in scripts or CI/CD pipelines, use the --yes flag to automatically accept all prompts:\n#!/bin/bash # Automated daily ingest script srake ingest --auto --yes --quiet","output-formats-for-scripts#Output Formats for Scripts":"Use structured output formats for easier parsing:\n# JSON for complex processing srake search \"mouse\" --format json | jq '.[] | .accession' # TSV for simple column extraction srake search \"human\" --format tsv --no-header | awk '{print $1}' # CSV for spreadsheet tools srake convert SRP123456 --to GSE --format csv \u003e results.csv","parallel-processing#Parallel Processing":"Leverage GNU parallel for large-scale processing:\n# Download multiple accessions in parallel cat accessions.txt | parallel -j 4 srake download {} --yes --quiet # Convert accessions in parallel parallel -j 8 srake convert {} --to GSE ::: SRP001 SRP002 SRP003","pipeline-composition-with-stdin#Pipeline Composition with stdin":"srake commands support stdin input, making them perfect for Unix pipelines:","shell-functions#Shell Functions":"Create helpful shell functions for common tasks:\n# Add to ~/.bashrc or ~/.zshrc # Quick SRA to GEO conversion sra2geo() { echo \"$1\" | srake convert --to GSE --quiet | tail -1 } # Download helper with defaults sra_download() { srake download \"$@\" --type fastq --parallel 4 --yes } # Search and count results sra_count() { srake search \"$1\" --format tsv --no-header | wc -l } # Usage $ sra2geo SRP123456 GSE98765 $ sra_download SRR123456 SRR123457 # Downloads with optimized settings $ sra_count \"homo sapiens RNA-Seq\" 1234"},"title":"Automation \u0026 Scripting"},"/nishad/srake/docs/compatibility/":{"data":{"1-streaming-architecture#1. Streaming Architecture":"Process 14GB+ files with minimal RAM No need to extract archives to disk Zero-copy data transfer","2-checkpoint-system#2. Checkpoint System":"Resume from exact interruption point Track progress across sessions No duplicate processing","3-integrated-filtering#3. Integrated Filtering":"Filter during ingestion, not after Reduce database size Faster subsequent queries","4-unified-cli#4. Unified CLI":"Single tool for all operations Consistent command structure No language-specific setup","advanced-feature-mapping#Advanced Feature Mapping":"","api-endpoints#API Endpoints":"For tools that need programmatic access, srake provides REST API equivalents:\n# Start API server srake server --port 8080 # Query endpoints curl \"http://localhost:8080/api/search?q=cancer\" curl \"http://localhost:8080/api/metadata/SRP123456\" curl \"http://localhost:8080/api/convert?from=SRP123456\u0026to=GSE\"","batch-operations#Batch Operations":"Other tools often require scripting:\n# pysradb for acc in accession_list: db.sra_metadata(acc) srake provides native batch support:\n# Batch conversion srake convert --batch accessions.txt --to GSE # Batch download srake download --list runs.txt --parallel 4 # Batch metadata srake metadata SRX001 SRX002 SRX003 --format json","command-equivalents#Command Equivalents":"","compatibility-with-other-tools#Compatibility with Other Tools":"Compatibility with Other Toolssrake provides comprehensive functionality that matches and extends popular SRA metadata tools. This guide shows how srake commands map to equivalent operations in other tools.","conclusion#Conclusion":"srake combines the best features of existing tools while adding unique capabilities like streaming processing, checkpoint recovery, and integrated filtering. It provides a unified, efficient solution for SRA metadata management that scales from small queries to massive dataset processing.","feature-comparison-matrix#Feature Comparison Matrix":"Feature srake SRAdb ffq pysradb MetaSRA Local Database ✅ ✅ ❌ ✅ ❌ Streaming Processing ✅ ❌ ❌ ❌ ❌ Accession Conversion ✅ ✅ ✅ ✅ ✅ Multi-source Download ✅ ✅ ✅ ✅ ❌ Relationship Queries ✅ ✅ ❌ ✅ ❌ Batch Operations ✅ ✅ ✅ ✅ ❌ Resume Capability ✅ ❌ ❌ ✅ ❌ Filtering on Ingest ✅ ❌ ❌ ❌ ❌ REST API ✅ ❌ ❌ ❌ ✅ Aspera Support ✅ ✅ ❌ ✅ ❌","ffq--srake#ffq → srake":"MetadataDownloadMulti-DB ffq:\n# Get metadata for an accession ffq SRR123456 # Get metadata with specific depth ffq -l 2 GSE123456 # Save to JSON ffq -o metadata.json SRR123456 srake:\n# Get metadata for an accession srake metadata SRR123456 # Get related metadata srake studies SRR123456 --detailed # Save to JSON srake metadata SRR123456 --format json --output metadata.json ffq:\n# Get FTP links ffq --ftp SRR123456 # Get AWS links ffq --aws SRR123456 # Get GCP links ffq --gcp SRR123456 srake:\n# Download from FTP srake download SRR123456 --source ftp # Download from AWS srake download SRR123456 --source aws # Download from GCP srake download SRR123456 --source gcp ffq:\n# Query from multiple databases ffq GSE123456 # Queries GEO ffq SRR123456 # Queries SRA ffq ENCSR000EYA # Queries ENCODE srake:\n# Convert between databases srake convert GSE123456 --to SRP # GEO to SRA srake convert SRR123456 --to GSM # SRA to GEO # Direct metadata query srake metadata GSE123456 # Handles any accession type","filtering-capabilities#Filtering Capabilities":"Most tools require post-processing:\n# SRAdb - filter after retrieval data \u003c- getSRA(search_terms = \"*\") filtered \u003c- subset(data, organism == \"Homo sapiens\") srake filters during ingestion:\n# Filter at source - more efficient srake ingest --auto \\ --organisms \"homo sapiens\" \\ --platforms ILLUMINA \\ --strategies RNA-Seq \\ --min-reads 10000000","from-ffq#From ffq":"Metadata retrieval:\n# ffq focuses on links # srake provides full metadata srake metadata SRR123456 --detailed Download URLs:\n# ffq shows URLs # srake downloads directly srake download SRR123456 --dry-run # To see URLs srake download SRR123456 # To download","from-pysradb#From pysradb":"Python to CLI:\n# pysradb requires Python scripting # srake works from command line srake convert GSM123456 --to SRX DataFrame to formats:\n# pysradb returns DataFrames # srake supports multiple formats srake search \"query\" --format csv","from-sradb#From SRAdb":"Database setup:\n# SRAdb: Download SQLite file # srake: Ingest directly srake ingest --auto Query syntax:\n# SRAdb: SQL queries # srake: Simple CLI commands srake search \"your query\" Output formats:\n# Both support multiple formats srake search \"query\" --format json","migration-guide#Migration Guide":"","performance-comparison#Performance Comparison":"Operation srake SRAdb pysradb 14GB Archive Ingestion 15 min 45 min* 35 min* Memory Usage 200MB 8GB+ 4GB+ Resume Support ✅ ❌ ❌ Concurrent Processing ✅ ❌ Limited *Requires full extraction to disk first","pysradb--srake#pysradb → srake":"MetadataDownloadConversionSearch pysradb:\n# Get metadata from pysradb import SRAweb db = SRAweb() df = db.sra_metadata('SRP123456') # Detailed metadata df = db.sra_metadata('SRP123456', detailed=True) srake:\n# Get metadata srake metadata SRP123456 # Detailed metadata srake metadata SRP123456 --detailed --format json pysradb:\n# Download SRA files db.download(df, protocol='fasp') # Download with filters db.download(df, filter_by_library_strategy='RNA-Seq') srake:\n# Download with Aspera srake download SRP123456 --aspera # Download with filters (filter during ingest) srake ingest --auto --strategies RNA-Seq srake download SRP123456 pysradb:\n# GSM to SRP srp = db.gsm_to_srp(['GSM123456']) # SRP to GSE gse = db.srp_to_gse(['SRP123456']) # SRX to SRR srr = db.srx_to_srr(['SRX123456']) srake:\n# GSM to SRP srake convert GSM123456 --to SRP # SRP to GSE srake convert SRP123456 --to GSE # SRX to SRR srake runs SRX123456 pysradb:\n# Search by study results = db.search_by_study_title('cancer') # Search experiments results = db.search_sra_studies('breast cancer', max_results=100) srake:\n# Search studies srake search \"cancer\" --limit 100 # Search with filters srake search \"breast cancer\" --organism \"homo sapiens\" --limit 100","resume-and-recovery#Resume and Recovery":"Other tools typically lack resume:\n# pysradb - no built-in resume # If interrupted, must restart from beginning srake has intelligent resume:\n# Automatic resume from interruption srake ingest --file large_archive.tar.gz # If interrupted, rerun same command to resume","sradb-r-package--srake#SRAdb (R Package) → srake":"ConversionDownloadSearchMetadata SRAdb:\n# Convert SRP to GSE sraConvert(in_acc = \"SRP123456\", out_type = \"gse\") # Convert GSM to SRX sraConvert(in_acc = \"GSM123456\", out_type = \"srx\") srake:\n# Convert SRP to GSE srake convert SRP123456 --to GSE # Convert GSM to SRX srake convert GSM123456 --to SRX SRAdb:\n# Download SRA files getSRAfile(in_acc = \"SRR123456\", method = \"curl\") # Download FASTQ files getFASTQfile(in_acc = \"SRR123456\", srcType = \"ftp\") srake:\n# Download SRA files srake download SRR123456 # Download FASTQ files srake download SRR123456 --type fastq SRAdb:\n# Search metadata getSRA(search_terms = \"breast cancer\", out_types = c(\"study\", \"sample\", \"experiment\")) srake:\n# Search metadata srake search \"breast cancer\" --format json SRAdb:\n# Get SRA info getSRAinfo(in_acc = \"SRP123456\", sra_con = sra_con) srake:\n# Get metadata srake metadata SRP123456 --detailed","unique-srake-advantages#Unique srake Advantages":""},"title":"Tool Compatibility"},"/nishad/srake/docs/examples/":{"data":{"advanced-filtering#Advanced Filtering":"","aggregation-and-analytics#Aggregation and Analytics":"Analyze metadata distributions:\n# Count studies by organism srake search \"RNA-Seq\" --aggregate-by organism # Get faceted results srake search \"cancer\" --facets --format json | \\ jq '.facets.platform' | head -20 # Count total matching records srake search \"single cell\" --count-only # Group by library strategy srake search --organism \"homo sapiens\" --aggregate-by library_strategy","automation-scripts#Automation Scripts":"","batch-operations#Batch Operations":"","building-a-local-index#Building a Local Index":"Create a searchable index of specific data types:\n# 1. Ingest filtered data srake ingest --auto \\ --organisms \"homo sapiens,mus musculus\" \\ --strategies \"RNA-Seq,ChIP-Seq,ATAC-Seq\" \\ --min-reads 10000000 \\ --date-from 2023-01-01 # 2. Export metadata for indexing srake search \"*\" --limit 0 --format json \u003e all_metadata.json # 3. Start API server for queries srake server --port 8080 \u0026 # 4. Query via API curl \"http://localhost:8080/api/search?q=transcription+factor\u0026limit=20\"","bulk-download-with-filtering#Bulk Download with Filtering":"Download only high-quality runs from multiple experiments:\n# Get runs with quality metrics srake runs SRP123456 --detailed --format json | \\ jq '.[] | select(.total_bases \u003e 10000000000) | .run_accession' \u003e \\ high_quality_runs.txt # Download filtered runs srake download --list high_quality_runs.txt \\ --source aws \\ --parallel 4 \\ --threads 2","composable-commands#Composable Commands":"srake commands can be chained together using Unix pipes:\n# Find experiments → Get runs → Download srake search \"CRISPR\" --format tsv --no-header | \\ cut -f1 | \\ xargs -I {} srake runs {} --format tsv --no-header | \\ cut -f1 | \\ srake download --type fastq # Convert accessions in bulk cat geo_accessions.txt | \\ srake convert --to SRP --format tsv --no-header | \\ cut -f2 \u003e sra_projects.txt # Chain multiple conversions echo \"GSE123456\" | \\ srake convert --to SRP | \\ grep SRP | \\ srake runs --format json","converting-a-list-of-accessions#Converting a List of Accessions":"Convert multiple accessions from a publication supplementary table:\n# Create accession list cat \u003e geo_accessions.txt \u003c\u003c EOF GSE111111 GSE222222 GSE333333 GSE444444 EOF # Batch convert to SRA projects srake convert --batch geo_accessions.txt \\ --to SRP \\ --format json \\ --output sra_projects.json # Extract just the SRP IDs cat sra_projects.json | jq -r '.[] | select(.error == null) | .targets[]' \u003e srp_list.txt","creating-a-download-queue#Creating a Download Queue":"Generate and process a download queue:\n#!/bin/bash # download_queue.sh # Get all RNA-Seq runs from 2024 srake search \"RNA-Seq\" \\ --format json \\ --date-from 2024-01-01 | \\ jq -r '.results[].accession' \u003e rna_seq_2024.txt # Process in batches of 10 split -l 10 rna_seq_2024.txt batch_ # Download each batch for batch in batch_*; do echo \"Processing $batch\" srake download --list $batch \\ --parallel 2 \\ --output ./downloads/ sleep 60 # Pause between batches done","cross-referencing-multiple-studies#Cross-referencing Multiple Studies":"Compare samples across different studies:\n# Get samples from multiple studies srake samples SRP001 --format json \u003e study1_samples.json srake samples SRP002 --format json \u003e study2_samples.json srake samples SRP003 --format json \u003e study3_samples.json # Or in batch for study in SRP001 SRP002 SRP003; do srake samples $study --detailed --format json \u003e ${study}_samples.json done","daily-report-generator#Daily Report Generator":"#!/bin/bash # daily_report.sh DATE=$(date +%Y-%m-%d) REPORT=\"report_$DATE.html\" cat \u003e $REPORT \u003c\u003c EOF SRA Daily Report - $DATE SRA Daily Report Generated: $(date)\nEOF # Database statistics echo \"Database Statistics\" \u003e\u003e $REPORT srake db info \u003e\u003e $REPORT echo \"\" \u003e\u003e $REPORT # New studies today echo \"New Studies\" \u003e\u003e $REPORT srake search \"*\" --date-from $DATE --format table \u003e\u003e $REPORT echo \"\" \u003e\u003e $REPORT echo \"\" \u003e\u003e $REPORT # Email report mail -s \"SRA Daily Report $DATE\" \\ -a $REPORT \\ team@example.com \u003c /dev/null","data-discovery#Data Discovery":"","debugging-failed-downloads#Debugging Failed Downloads":"# Dry run to check URLs srake download SRR123456 --dry-run --verbose # Test different sources for source in ftp aws gcp ncbi; do echo \"Testing $source\" srake download SRR123456 \\ --source $source \\ --dry-run done # Use verbose mode for debugging srake download SRR123456 \\ --verbose \\ --retry 5","downloading-data-for-a-published-study#Downloading Data for a Published Study":"Download all FASTQ files for a study mentioned in a paper:\n# Convert GEO accession from paper to SRA srake convert GSE123456 --to SRP # Get all runs for the study srake runs SRP123456 --format json --output runs.json # Download all runs in parallel srake download SRP123456 \\ --type fastq \\ --source aws \\ --parallel 4 \\ --output ./fastq_files/","examples-and-use-cases#Examples and Use Cases":"Examples and Use CasesReal-world examples demonstrating common workflows with srake, including pipeline composition and automation patterns.","exploring-platform-specific-data#Exploring Platform-Specific Data":"Find all Oxford Nanopore sequencing data:\n# Ingest only Nanopore data srake ingest --auto \\ --platforms OXFORD_NANOPORE # Search for specific applications srake search \"metagenome\" \\ --platform OXFORD_NANOPORE \\ --limit 50","finding-related-experiments#Finding Related Experiments":"Discover all experiments related to a sample:\n# Start with a sample accession SAMPLE=\"SRS123456\" # Get all experiments for this sample srake experiments $SAMPLE --detailed # Get the parent study srake studies $SAMPLE # Get all other samples in the study STUDY=$(srake studies $SAMPLE --format json | jq -r '.[0].study_accession') srake samples $STUDY --detailed","finding-rna-seq-data-for-a-specific-organism#Finding RNA-Seq Data for a Specific Organism":"Find all human RNA-Seq experiments published in 2024:\n# Build search index first srake search index --build # Search with advanced query syntax srake search \"organism:human AND library_strategy:RNA-Seq\" --advanced \\ --date-from 2024-01-01 \\ --limit 100 \\ --format csv \\ --output human_rna_seq_2024.csv # Search for specific disease studies srake search \"breast cancer AND organism:\\\"homo sapiens\\\" AND library_strategy:RNA-Seq\" \\ --advanced \\ --spots-min 10000000 \\ --format json \\ --output breast_cancer_studies.json","handling-large-result-sets#Handling Large Result Sets":"# Paginate through large results OFFSET=0 LIMIT=1000 while true; do COUNT=$(srake search \"human\" \\ --offset $OFFSET \\ --limit $LIMIT \\ --format json | \\ jq '.results | length') if [ $COUNT -eq 0 ]; then break fi # Process batch srake search \"human\" \\ --offset $OFFSET \\ --limit $LIMIT \\ --format json \u003e batch_$OFFSET.json OFFSET=$((OFFSET + LIMIT)) done","incremental-updates#Incremental Updates":"Keep your database current with minimal overhead:\n#!/bin/bash # daily_update.sh # Check last update LAST_UPDATE=$(srake db info | grep \"Last update\" | cut -d: -f2) # Ingest only new data srake ingest --daily \\ --date-from \"$LAST_UPDATE\" \\ --no-progress # Log update echo \"$(date): Updated from $LAST_UPDATE\" \u003e\u003e update.log","integration-examples#Integration Examples":"","metadata-analysis-pipeline#Metadata Analysis Pipeline":"Extract and analyze metadata for a research domain:\n# Get all cancer-related studies srake search \"cancer\" --format json --limit 1000 \u003e cancer_studies.json # Extract platform distribution cat cancer_studies.json | \\ jq -r '.results[].platform' | \\ sort | uniq -c | sort -rn # Get temporal distribution cat cancer_studies.json | \\ jq -r '.results[].published' | \\ cut -d'-' -f1 | \\ sort | uniq -c","multi-criteria-filtering#Multi-criteria Filtering":"Complex filtering for specific research needs:\n# Ingest single-cell RNA-seq from human brain srake ingest --auto \\ --taxon-ids 9606 \\ --organisms \"homo sapiens\" \\ --strategies \"RNA-Seq\" \\ --min-reads 100000000 \\ --filter-verbose # Search with additional criteria srake search \"brain OR neuron OR glia\" \\ --format json | \\ jq '.results[] | select(.title | test(\"single.cell|sc.?RNA|10x\"; \"i\"))'","next-steps#Next Steps":"Review the CLI Reference for detailed command options Explore the API Documentation for programmatic access Check Performance Tips for optimization strategies","parallel-processing#Parallel Processing":"Maximize throughput with parallel operations:\n# Parallel conversion cat accessions.txt | \\ parallel -j 4 'srake convert {} --to GSE --format json \u003e {}.json' # Parallel metadata fetch cat studies.txt | \\ parallel -j 8 'srake metadata {} --format json' \u003e all_metadata.jsonl # Parallel download with resource limits nice -n 10 srake download --list large_dataset.txt \\ --parallel 4 \\ --threads 2 \\ --output /data/downloads/","performance-optimization#Performance Optimization":"","quality-control-pipeline#Quality Control Pipeline":"Filter and validate high-quality datasets:\n# Function to check data quality check_quality() { local accession=$1 # Get run information srake runs $accession --format json | \\ jq -r '.[] | \"\\(.run_accession):\\(.total_bases):\\(.total_spots)\"' | \\ while IFS=: read -r run bases spots; do if [ $bases -gt 10000000000 ]; then echo \"$run PASS\" else echo \"$run FAIL\" fi done } # Check multiple studies for study in SRP001 SRP002 SRP003; do echo \"Checking $study\" check_quality $study done","research-workflows#Research Workflows":"","stream-processing#Stream Processing":"Process large datasets without intermediate files:\n# Real-time filtering and conversion srake search \"RNA-Seq\" --format tsv --no-header | \\ awk '$3 \u003e 1000000' | \\ cut -f1 | \\ while read acc; do srake convert $acc --to GSE --quiet done # Parallel processing with xargs srake search \"mouse\" --limit 100 --format tsv --no-header | \\ cut -f1 | \\ xargs -P 4 -I {} srake metadata {} --format json --quiet","troubleshooting-examples#Troubleshooting Examples":"","unix-pipeline-integration#Unix Pipeline Integration":"","using-advanced-search-features#Using Advanced Search Features":"Leverage boolean operators and field-specific queries:\n# Complex boolean queries srake search \"(cancer OR tumor) AND organism:human NOT cell_type:hela\" --advanced # Wildcard searches srake search \"RNA* AND platform:ILLUMINA\" --advanced # Range queries for high-coverage data srake search \"spots:[10000000 TO *] AND bases:[1000000000 TO *]\" --advanced # Fuzzy search for typo tolerance srake search \"transciptome\" --fuzzy # Finds \"transcriptome\""},"title":"Examples"},"/nishad/srake/docs/features/":{"data":{"":"","architecture#Architecture":"Explore the powerful features that make srake the fastest and most efficient SRA metadata processor.\nCore Features Filtering System Process only the data you need with smart filters for taxonomy, organism, platform, date ranges, and quality metrics.\nResume Capability Handle interruptions gracefully with automatic checkpoint system and progress tracking.\nPerformance Optimizations that enable processing 20,000+ records per second with minimal memory usage.\nArchitecture Zero-copy streaming design that processes multi-gigabyte files without intermediate storage.","core-features#Core Features":"","filtering-system#Filtering System":"","performance#Performance":"","resume-capability#Resume Capability":""},"title":"Features"},"/nishad/srake/docs/features/filtering/":{"data":{"":"The filtering system allows you to process only the data you need, reducing storage requirements.","best-practices#Best Practices":"","common-patterns#Common Patterns":"Model OrganismsFocus on well-studied species for comparative analysis Recent DataFilter by date for the latest sequencing technologies High QualityUse quality filters for publication-ready datasets Technology SpecificFilter by platform for consistent processing pipelines","complex-filter-combinations#Complex Filter Combinations":"","date-range-filtering#Date Range Filtering":"Filter by submission or publication dates:\nYear 2024Last 90 DaysHistorical Data srake ingest --file archive.tar.gz \\ --date-from 2024-01-01 \\ --date-to 2024-12-31 srake ingest --file archive.tar.gz \\ --date-from 2024-10-01 srake ingest --file archive.tar.gz \\ --date-to 2020-12-31","filter-configuration-files#Filter Configuration Files":"For complex, reusable filter sets, use YAML configuration:\nconfig.yamlUsage # filters.yaml taxonomy: include: [9606, 10090] exclude: [562] platforms: - ILLUMINA - OXFORD_NANOPORE strategies: - RNA-Seq - WGS date: from: \"2024-01-01\" to: \"2024-12-31\" quality: min_reads: 10000000 min_bases: 1000000000 # Use configuration file srake ingest --file archive.tar.gz \\ --filter-config filters.yaml # Override specific settings srake ingest --file archive.tar.gz \\ --filter-config filters.yaml \\ --date-from 2025-01-01","filter-processing#Filter Processing":"Filters are applied during the streaming pipeline, allowing efficient processing of large datasets without loading everything into memory.","filter-types#Filter Types":"","filters-not-working#Filters Not Working?":"### Verify Filter Syntax ```bash # Check your command srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --verbose ``` ### Check Statistics ```bash # Use stats-only mode srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --stats-only ``` ### Enable Verbose Output ```bash # See detailed filtering decisions srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --verbose --log-level debug ```","library-strategy-filtering#Library Strategy Filtering":"Filter by experimental strategy:\nRNA-SeqGenomicsEpigenomicsMultiple # RNA sequencing only srake ingest --file archive.tar.gz \\ --strategies RNA-Seq # Whole genome sequencing srake ingest --file archive.tar.gz \\ --strategies WGS,WXS # Epigenomics studies srake ingest --file archive.tar.gz \\ --strategies ChIP-Seq,ATAC-Seq,Bisulfite-Seq # Multiple strategies srake ingest --file archive.tar.gz \\ --strategies RNA-Seq,WGS,ChIP-Seq Common Strategies:\nRNA-Seq - RNA sequencing WGS - Whole Genome Sequencing WXS - Whole Exome Sequencing ChIP-Seq - Chromatin IP ATAC-Seq - Chromatin accessibility Bisulfite-Seq - DNA methylation Hi-C - Chromosome conformation","next-steps#Next Steps":"Resume CapabilityHandle interruptions gracefully PerformanceOptimization techniques ExamplesReal-world filtering scenarios","organism-name-filtering#Organism Name Filtering":"Filter by scientific names when you don’t know the taxonomy IDs:\n# Single organism srake ingest --file archive.tar.gz \\ --organisms \"homo sapiens\" # Multiple organisms srake ingest --file archive.tar.gz \\ --organisms \"homo sapiens,mus musculus,rattus norvegicus\"","overview#Overview":"The filtering system operates during the streaming pipeline, applying filters before data is inserted into the database:\nKey features:\nMemory Efficient: Filters applied during streaming Early Rejection: Unwanted records discarded before database operations Statistics: Track filtering effectiveness","platform-filtering#Platform Filtering":"IlluminaLong ReadsMultipleAll Platforms # Illumina data only srake ingest --file archive.tar.gz \\ --platforms ILLUMINA # Long-read platforms srake ingest --file archive.tar.gz \\ --platforms OXFORD_NANOPORE,PACBIO_SMRT # Multiple platforms srake ingest --file archive.tar.gz \\ --platforms ILLUMINA,ION_TORRENT Available Platforms:\nILLUMINA OXFORD_NANOPORE PACBIO_SMRT ION_TORRENT LS454 ABI_SOLID COMPLETE_GENOMICS","preview-mode#Preview Mode":"Test your filters without inserting data:\n### Run with --stats-only ```bash srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --platforms ILLUMINA \\ --strategies RNA-Seq \\ --stats-only ``` ### Review Statistics ``` Filter Statistics ───────────────── Total XML files: 150,234 Files matching filters: 12,456 (8.3%) Records that would be inserted: Studies: 3,234 Experiments: 12,456 Samples: 11,234 Runs: 15,678 Estimated database size: 1.2 GB Processing time estimate: 15 minutes ``` ### Adjust and Apply ```bash # Apply the filters for real srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --platforms ILLUMINA \\ --strategies RNA-Seq ```","quality-filtering#Quality Filtering":"Filter by sequencing depth and quality:\nHigh CoverageSpecific RangeUltra Deep # Minimum 10M reads and 1GB bases srake ingest --file archive.tar.gz \\ --min-reads 10000000 \\ --min-bases 1000000000 # Between 5M and 50M reads srake ingest --file archive.tar.gz \\ --min-reads 5000000 \\ --max-reads 50000000 # Ultra-deep sequencing (10GB+ bases) srake ingest --file archive.tar.gz \\ --min-bases 10000000000","research-specific-workflows#Research-Specific Workflows":"Cancer ResearchPopulation GeneticsMicrobiomeSingle Cell # Human cancer RNA-Seq studies srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --strategies RNA-Seq,WGS \\ --date-from 2023-01-01 \\ --min-reads 20000000 # Population genomics data srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --strategies WGS \\ --platforms ILLUMINA \\ --min-bases 30000000000 # Microbiome studies srake ingest --file archive.tar.gz \\ --strategies AMPLICON,WGS \\ --platforms ILLUMINA,ION_TORRENT \\ --exclude-taxon-ids 9606,10090 # Single-cell RNA-Seq srake ingest --file archive.tar.gz \\ --taxon-ids 9606,10090 \\ --strategies RNA-Seq \\ --date-from 2022-01-01 \\ --platforms ILLUMINA","taxonomy-filtering#Taxonomy Filtering":"Filter by NCBI taxonomy IDs to focus on specific organisms:\nSingle SpeciesMultiple SpeciesExclude TaxaCombined # Human data only (taxonomy ID 9606) srake ingest --file archive.tar.gz \\ --taxon-ids 9606 # Human, mouse, and zebrafish srake ingest --file archive.tar.gz \\ --taxon-ids 9606,10090,7955 # Exclude viruses and bacteria srake ingest --file archive.tar.gz \\ --exclude-taxon-ids 32630,2697049,562 # Mammals excluding E. coli contamination srake ingest --file archive.tar.gz \\ --taxon-ids 9606,10090 \\ --exclude-taxon-ids 562 Common Taxonomy IDs:\n9606 - Homo sapiens (human) 10090 - Mus musculus (mouse) 7955 - Danio rerio (zebrafish) 7227 - Drosophila melanogaster 562 - Escherichia coli","tips-for-effective-filtering#Tips for Effective Filtering":"Start with –stats-only to preview results Use taxonomy filters for targeted datasets Combine filters for precise data selection Save configurations for reproducible workflows Monitor statistics to verify filter effectiveness","troubleshooting#Troubleshooting":""},"title":"Filtering System"},"/nishad/srake/docs/features/resume/":{"data":{"":"srake includes intelligent resume functionality that handles interruptions gracefully, allowing you to continue processing from where you left off.","architecture#Architecture":"","automatic-resume#Automatic Resume":"Simply run the same command again after interruption:\n# Original command srake ingest --file NCBI_SRA_Full_20250818.tar.gz # If interrupted, run again srake ingest --file NCBI_SRA_Full_20250818.tar.gz # Output: Previous ingestion found: Source: NCBI_SRA_Full_20250818.tar.gz Progress: 45.3% complete (6.3GB/14GB) Records: 1,234,567 processed Started: 2025-01-17 10:30:00 Resume from last position? (y/n): y Resuming from: experiment_batch_042.xml [====================\u003e.................] 45.3% | 6.3GB/14GB | ETA: 15 min","batch-processing#Batch Processing":"# Process multiple archives with resume support for archive in *.tar.gz; do srake ingest --file \"$archive\" # Each file has independent resume tracking done","best-practices#Best Practices":"Let It Resume: Don’t use --force unless necessary Regular Checkpoints: Default (1000 records) works well for most cases Monitor Progress: Use --verbose to see detailed progress Keep Database: Don’t delete metadata.db during processing Network Stability: For large files, ensure stable connection","check-status#Check Status":"View current or last ingestion status:\nsrake ingest --status # Output: Current Ingestion Status ──────────────────────── Source: NCBI_SRA_Full_20250818.tar.gz State: In Progress Progress: 67.8% complete Records Processed: 2,345,678 Start Time: 2025-01-17 10:30:00 Last Update: 2025-01-17 11:45:23 Estimated Time Remaining: 12 minutes","cleanup-policy#Cleanup Policy":"Progress records kept for 30 days after completion Failed attempts kept for 7 days Manual cleanup available via --cleanup","configure-checkpoints#Configure Checkpoints":"Adjust checkpoint frequency for your needs:\n# Checkpoint every 5000 records (less frequent) srake ingest --file archive.tar.gz --checkpoint 5000 # Checkpoint every 100 records (more frequent, safer) srake ingest --file archive.tar.gz --checkpoint 100","corrupted-progress#Corrupted Progress":"If progress records are corrupted:\n# Clean up progress records srake ingest --cleanup # Start fresh srake ingest --file archive.tar.gz","examples#Examples":"","file-identification#File Identification":"Files are identified by:\nSource URL/path SHA-256 hash of first 1MB File size Modification time","filtered-resume#Filtered Resume":"# Complex filter with resume srake ingest --file huge_archive.tar.gz \\ --taxon-ids 9606,10090 \\ --platforms ILLUMINA \\ --strategies RNA-Seq,WGS \\ --date-from 2024-01-01 \\ --min-reads 10000000 # Power failure at 60% # Resume with exact same filters srake ingest --file huge_archive.tar.gz \\ --taxon-ids 9606,10090 \\ --platforms ILLUMINA \\ --strategies RNA-Seq,WGS \\ --date-from 2024-01-01 \\ --min-reads 10000000 # Continues from 60% with filters intact","force-fresh-start#Force Fresh Start":"Override existing progress and start from beginning:\nsrake ingest --file archive.tar.gz --force # Warning shown: ⚠️ Existing progress will be discarded Continue? (y/n): y Starting fresh ingestion...","how-it-works#How It Works":"","interactive-mode#Interactive Mode":"Get prompted before resuming:\nsrake ingest --file archive.tar.gz --interactive # Always prompts: Previous ingestion found. Resume? (y/n):","key-features#Key Features":"Automatic Progress Tracking: Real-time tracking of download and processing progress Database-Backed Persistence: Progress stored in SQLite for reliability Checkpoint System: Periodic checkpoints for accurate recovery points File-Level Deduplication: Skip already-processed XML files on resume HTTP Range Support: Resume downloads from exact byte position Smart Recovery: Automatically detect and resume interrupted sessions","network-issues#Network Issues":"For unstable networks:\n# Increase retry attempts srake ingest --file https://ftp.ncbi.nlm.nih.gov/sra/archive.tar.gz \\ --max-retries 10 \\ --retry-delay 30","next-steps#Next Steps":"Learn about Performance Optimizations Explore Architecture Details See Real-World Examples","overview#Overview":"Processing large SRA metadata archives (14GB+) can take significant time. Network issues, system crashes, or user interruptions can occur during processing. srake’s resume capability ensures you never have to start over from the beginning.","performance-impact#Performance Impact":"Resume capability adds minimal overhead:\nAspect Impact Memory Usage \u003c 1MB for progress tracking Processing Speed \u003c 5% reduction Checkpoint Time \u003c 100ms per checkpoint Resume Time \u003c 5 seconds to restart Database Size \u003c 100KB for progress records","progress-database-schema#Progress Database Schema":"CREATE TABLE ingest_progress ( id INTEGER PRIMARY KEY, source_url TEXT NOT NULL, source_hash TEXT UNIQUE NOT NULL, total_bytes INTEGER, downloaded_bytes INTEGER, processed_bytes INTEGER, last_tar_position INTEGER, last_xml_file TEXT, records_processed INTEGER, state TEXT, started_at TIMESTAMP, updated_at TIMESTAMP, completed_at TIMESTAMP, error_message TEXT ); CREATE TABLE processed_files ( id INTEGER PRIMARY KEY, progress_id INTEGER, filename TEXT NOT NULL, processed_at TIMESTAMP, FOREIGN KEY (progress_id) REFERENCES ingest_progress(id) );","progress-tracking#Progress Tracking":"srake tracks multiple aspects of progress:\nDownload Progress\nBytes downloaded vs. total size HTTP range support for partial downloads Network failure recovery Processing Progress\nCurrent tar position in archive Last processed XML file Records inserted into database Checkpoint System\nPeriodic checkpoints (default: every 1000 records) Safe points for recovery Minimal performance impact (\u003c 100ms)","recovery-process#Recovery Process":"Validation Phase\nVerify source file exists/accessible Check source hash matches Validate database consistency Restoration Phase\nSeek to last tar position Skip processed XML files Restore counters and statistics Continuation Phase\nResume normal processing Continue checkpoint creation Update progress tracking","research-workflow#Research Workflow":"# Start large ingestion Friday evening srake ingest --monthly \\ --taxon-ids 9606 \\ --strategies RNA-Seq # System maintenance interrupts at 30% # Resume Monday morning srake ingest --monthly \\ --taxon-ids 9606 \\ --strategies RNA-Seq # Continues from 30%, completes successfully","resume-detection#Resume Detection":"When you run an ingest command, srake automatically:\nChecks for existing progress records Validates the source matches Offers to resume or start fresh Resumes from the last safe checkpoint","resume-not-working#Resume Not Working":"If resume doesn’t work as expected:\nCheck source file hasn’t changed\n# View progress details srake ingest --status Verify database integrity\n# Database location ls -la ./data/metadata.db Force restart if needed\nsrake ingest --file archive.tar.gz --force","resume-with-filters#Resume with Filters":"Resume works seamlessly with filtering:\n# Original filtered ingestion srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --platforms ILLUMINA # Resume with same filters applied srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --platforms ILLUMINA # Filters are preserved and reapplied","state-management#State Management":"Progress states:\npending: Initialized but not started downloading: Downloading from remote source processing: Processing tar archive completed: Successfully finished failed: Error occurred cancelled: User cancelled","technical-details#Technical Details":"","troubleshooting#Troubleshooting":"","using-resume#Using Resume":""},"title":"Resume Capability"},"/nishad/srake/docs/features/search/":{"data":{"accession-list#Accession List":"# Just accession numbers (useful for batch downloads) srake search \"RNA-Seq human\" --format accession \u003e accessions.txt # Pipe to other commands srake search \"single cell\" --format accession | head -20 | xargs srake metadata","advanced-query-syntax#Advanced Query Syntax":"Enable advanced queries with the --advanced flag:","advanced-search-capabilities#Advanced Search Capabilities":"Advanced Search CapabilitiesSRake provides powerful full-text search with advanced query syntax, comprehensive filtering, and aggregation capabilities that match or exceed leading SRA tools.","aggregate-by-field#Aggregate by Field":"# Group by organism srake search \"RNA-Seq\" --aggregate-by organism # Output: # Aggregation by organism # ────────────────────────────────────────────────── # Homo sapiens 15234 # Mus musculus 8921 # Rattus norvegicus 3456 # ... # Aggregate by platform srake search --aggregate-by platform # Aggregate by library strategy srake search \"cancer\" --aggregate-by library_strategy","aggregation--analytics#Aggregation \u0026amp; Analytics":"","api-access#API Access":"The search functionality is also available via the REST API:\n# Start the server srake server --port 8080 # Search via API curl \"localhost:8080/api/search?q=human\u0026limit=10\" # With filters curl \"localhost:8080/api/search?q=cancer\u0026organism=homo+sapiens\u0026platform=ILLUMINA\" # JSON response curl -H \"Accept: application/json\" \"localhost:8080/api/search?q=RNA-Seq\"","biological-filters#Biological Filters":"# Organism srake search --organism \"homo sapiens\" srake search --organism \"mus musculus\" # Platform and instrument srake search --platform ILLUMINA srake search --platform OXFORD_NANOPORE srake search --instrument \"Illumina HiSeq 2500\" # Library details srake search --library-strategy RNA-Seq srake search --library-strategy ChIP-Seq srake search --library-source TRANSCRIPTOMIC srake search --library-selection RANDOM srake search --library-layout PAIRED # Study type srake search --study-type \"Transcriptome Analysis\"","boolean-operators#Boolean Operators":"# AND operator (both conditions must match) srake search \"organism:human AND library_strategy:RNA-Seq\" --advanced # OR operator (either condition matches) srake search \"platform:ILLUMINA OR platform:PACBIO\" --advanced # NOT operator (exclude matches) srake search \"cancer NOT organism:mouse\" --advanced # Complex combinations srake search \"(organism:human OR organism:mouse) AND library_strategy:RNA-Seq NOT cell_type:hela\" --advanced","cancer-research#Cancer Research":"# All cancer studies srake search \"cancer OR tumor OR tumour OR carcinoma\" --advanced # Breast cancer specifically srake search \"\\\"breast cancer\\\" OR BRCA1 OR BRCA2\" --advanced # Human cancer RNA-Seq srake search \"cancer AND organism:human AND library_strategy:RNA-Seq\" --advanced","combining-filters#Combining Filters":"# Complex filter combinations srake search \"cancer\" \\ --organism \"homo sapiens\" \\ --platform ILLUMINA \\ --library-strategy RNA-Seq \\ --spots-min 1000000 \\ --date-from 2023-01-01 # All RNA-Seq experiments from human samples with high coverage srake search \\ --organism \"homo sapiens\" \\ --library-strategy RNA-Seq \\ --library-layout PAIRED \\ --spots-min 10000000","comparison-with-other-tools#Comparison with Other Tools":"Feature SRake SRAdb pysradb ffq NCBI E-utilities Full-text search ✅ Fast ✅ Slow ✅ Medium ❌ ✅ Online Boolean operators ✅ ❌ ❌ ❌ ✅ Field queries ✅ ⚠️ ⚠️ ❌ ✅ Wildcards ✅ ⚠️ ❌ ❌ ⚠️ Range queries ✅ ❌ ❌ ❌ ✅ Fuzzy search ✅ ❌ ❌ ❌ ❌ Aggregations ✅ ⚠️ ✅ ❌ ⚠️ Offline mode ✅ ✅ ❌ ❌ ❌ Response time \u003c50ms \u003e1s ~500ms N/A \u003e1s Rate limits None None None None Yes","comprehensive-filtering#Comprehensive Filtering":"","count-results#Count Results":"# Get total count only srake search \"cancer\" --count-only # Output: 42156 # Count with filters srake search --organism \"homo sapiens\" --platform ILLUMINA --count-only # JSON output for scripts srake search \"RNA-Seq\" --count-only --format json","csvtsv#CSV/TSV":"# CSV format srake search \"human\" --format csv \u003e results.csv # TSV format srake search \"mouse\" --format tsv \u003e results.tsv # Without headers srake search \"cancer\" --format csv --no-header","custom-fields#Custom Fields":"# Select specific fields srake search \"human\" --fields \"accession,organism,platform,spots\" # Export specific fields as CSV srake search \"RNA-Seq\" --fields \"accession,title,organism\" --format csv","date-range-filters#Date Range Filters":"# Date range filtering srake search --date-from 2023-01-01 --date-to 2023-12-31 srake search --date-from 2022-06-01 srake search --date-to 2024-01-01","exact-matching#Exact Matching":"# Require exact phrase matching srake search \"RNA sequencing\" --exact","examples-by-use-case#Examples by Use Case":"","export-to-file#Export to File":"# Save results to file srake search \"cancer\" --output results.json --format json srake search \"RNA-Seq\" --output results.csv --format csv # Large exports srake search \"human\" --limit 10000 --output human_studies.json --format json","faceted-search#Faceted Search":"# Show facets (counts by category) srake search \"human\" --facets # Facets with JSON output for analysis srake search \"cancer\" --facets --format json","field-specific-searches#Field-Specific Searches":"Target specific metadata fields:\n# Search in specific fields srake search \"title:breast cancer\" --advanced srake search \"organism:\\\"homo sapiens\\\"\" --advanced srake search \"platform:ILLUMINA\" --advanced # Field aliases for convenience srake search \"org:human lib:RNA-Seq plat:ILLUMINA\" --advanced Available field aliases:\norg → organism plat → platform lib → library_strategy strat → library_strategy inst → instrument_model acc → accession","finding-rna-seq-studies#Finding RNA-Seq Studies":"# All human RNA-Seq studies srake search \"organism:human AND library_strategy:RNA-Seq\" --advanced # High-quality RNA-Seq (\u003e10M reads) srake search --library-strategy RNA-Seq --spots-min 10000000 # Recent RNA-Seq studies srake search --library-strategy RNA-Seq --date-from 2023-01-01","full-text-search-with-bleve#Full-Text Search with Bleve":"SRake uses the Bleve search engine for lightning-fast local search:\nNo network dependency: All searches run locally Sub-second response: Most queries complete in \u003c50ms Automatic indexing: Index updates during data ingestion Fuzzy matching: Tolerates typos and variations","fuzzy-search#Fuzzy Search":"Tolerates typos and variations:\n# Fuzzy search for typo tolerance srake search \"hmuan\" --fuzzy # Finds \"human\" srake search \"cancre\" --fuzzy # Finds \"cancer\" srake search \"rnaseq\" --fuzzy # Finds \"RNA-Seq\"","index-not-found#Index not found":"# Build the index srake search index --build","json#JSON":"# Pretty-printed JSON srake search \"cancer\" --format json # Pipe to jq for processing srake search \"RNA-Seq\" --format json | jq '.hits[].fields.organism' | sort | uniq -c","no-results#No results":"# Try fuzzy search srake search \"your query\" --fuzzy # Check available data srake search --stats","output-formats#Output Formats":"","pagination#Pagination":"# Pagination for large result sets srake search \"human\" --limit 100 --offset 0 # First 100 srake search \"human\" --limit 100 --offset 100 # Next 100 srake search \"human\" --limit 100 --offset 200 # Next 100","performance-options#Performance Options":"","phrase-searches#Phrase Searches":"Exact phrase matching with quotes:\n# Phrase searches srake search \"\\\"breast cancer\\\"\" --advanced srake search \"\\\"single cell RNA sequencing\\\"\" --advanced","quality-control#Quality Control":"# Find low-quality samples srake search --spots-max 100000 # Find high-coverage WGS srake search --library-strategy WGS --bases-min 30000000000 # Platform-specific searches srake search --platform PACBIO --library-strategy WGS","quick-start#Quick Start":"# Build the search index first srake search index --build # Basic search srake search \"homo sapiens\" # Advanced query srake search \"organism:human AND library_strategy:RNA-Seq\" --advanced # With filters srake search --organism \"homo sapiens\" --platform ILLUMINA","range-queries#Range Queries":"Search numeric and date ranges:\n# Numeric ranges srake search \"spots:[1000000 TO 5000000]\" --advanced srake search \"bases:[* TO 1000000000]\" --advanced srake search \"spots:[1000000 TO *]\" --advanced # Combined with other queries srake search \"organism:human AND spots:[1000000 TO *]\" --advanced","search-features#Search Features":"","search-index-management#Search Index Management":"# Build or rebuild index srake search index --build srake search index --rebuild # Custom batch size for large databases srake search index --build --batch-size 1000 # Parallel indexing srake search index --build --workers 4 # Verify index integrity srake search index --verify # Show index statistics srake search index --stats","search-modes#Search Modes":"","sequencing-metrics#Sequencing Metrics":"# Filter by spots (reads) srake search --spots-min 1000000 srake search --spots-min 1000000 --spots-max 10000000 # Filter by bases srake search --bases-min 1000000000 # 1 billion bases srake search --bases-max 50000000000 # 50 billion bases","single-cell-studies#Single-Cell Studies":"# Single-cell RNA-Seq srake search \"\\\"single cell\\\" OR scRNA-Seq OR \\\"10x Genomics\\\"\" --advanced # Recent single-cell studies srake search \"single cell\" --date-from 2023-01-01 --platform ILLUMINA","slow-searches#Slow searches":"# Rebuild index with optimization srake search index --rebuild --batch-size 1000","table-default#Table (Default)":"srake search \"homo sapiens\" --limit 10","tips--tricks#Tips \u0026amp; Tricks":"Build index after ingestion: Always rebuild the search index after ingesting new data Use field queries: More precise than general text search Combine filters: Multiple filters use AND logic Export for analysis: Use JSON format with jq for complex analysis Check facets: Use --facets to understand data distribution Fuzzy for variants: Use fuzzy search for organism names and technical terms","troubleshooting#Troubleshooting":"","wildcards#Wildcards":"Use * for multiple characters, ? for single character:\n# Wildcard searches srake search \"RNA*\" --advanced # Matches RNA-Seq, RNA-seq, RNAseq srake search \"hum?n\" --advanced # Matches human, humin srake search \"SRP*123\" --advanced # Matches any SRP ending in 123"},"title":"Advanced Search"},"/nishad/srake/docs/first-page/":{"data":{"":"A simple demo page."},"title":"Demo Page"},"/nishad/srake/docs/folder/":{"data":{"":"Pages can be organized into folders."},"title":"Folder"},"/nishad/srake/docs/folder/leaf/":{"data":{"":"This page is under a folder."},"title":"Leaf Page"},"/nishad/srake/docs/getting-started/":{"data":{"":"","automation-features#Automation Features":"srake follows clig.dev best practices for CLI design, making it perfect for automation:","configuration-options#Configuration Options":"","database-management#Database Management":"Database Info:\nsrake db info # Shows: # • Database size # • Table counts # • Index status Custom Location:\nsrake ingest \\ --file archive.tar.gz \\ --db /custom/path/db.sqlite Verbose Mode:\nsrake ingest \\ --file archive.tar.gz \\ --verbose","dry-run--debug#Dry Run \u0026amp; Debug":"# Preview actions without executing srake download SRP123456 --dry-run # Enable debug output for troubleshooting srake convert SRP123456 --to GSE --debug","filter-by-date-range#Filter by Date Range":"# Data from 2024 only srake ingest --file archive.tar.gz \\ --date-from 2024-01-01 \\ --date-to 2024-12-31","filter-by-platform--strategy#Filter by Platform \u0026amp; Strategy":"Illumina RNA-Seq:\nsrake ingest --file archive.tar.gz \\ --platforms ILLUMINA \\ --strategies RNA-Seq Oxford Nanopore WGS:\nsrake ingest --file archive.tar.gz \\ --platforms OXFORD_NANOPORE \\ --strategies WGS","filter-by-taxonomy#Filter by Taxonomy":"Single SpeciesMultiple SpeciesExclude Species # Human data only (taxonomy ID 9606) srake ingest --file archive.tar.gz \\ --taxon-ids 9606 # Human, mouse, and zebrafish srake ingest --file archive.tar.gz \\ --taxon-ids 9606,10090,7955 # Exclude viruses srake ingest --file archive.tar.gz \\ --exclude-taxon-ids 32630,2697049","filtering-options#Filtering Options":"Filtering helps reduce database size by processing only the data you need.","getting-help#Getting Help":"Need assistance? Check these resources:\n📚 FAQ 💬 GitHub Discussions 🐛 Report Issues 📖 Full Documentation","installation#Installation":"srake provides multiple installation methods to suit your needs:\nGoDockerBinarySource Requirements: Go 1.19 or later\ngo install github.com/nishad/srake/cmd/srake@latest Verify the installation:\nsrake --version Run in container:\n# Pull the image docker pull ghcr.io/nishad/srake:latest # Run with volume mount docker run -v $(pwd)/data:/data \\ ghcr.io/nishad/srake:latest \\ ingest --auto Download pre-built binaries:\n# Linux/macOS wget https://github.com/nishad/srake/releases/latest/download/srake-$(uname -s)-$(uname -m).tar.gz tar -xzf srake-*.tar.gz sudo mv srake /usr/local/bin/ Verify installation:\nsrake --version Build from source:\ngit clone https://github.com/nishad/srake.git cd srake go build -o srake ./cmd/srake ./srake --help","next-steps#Next Steps":"Filtering SystemLearn advanced filtering techniques Resume CapabilityHandle large files reliably API ReferenceProgrammatic access guide ExamplesReal-world use cases","non-interactive-mode#Non-Interactive Mode":"# Use --yes flag to skip all prompts srake ingest --auto --yes srake download SRP123456 --yes","performance-tuning#Performance Tuning":"CheckpointsProgressConcurrency # Adjust checkpoint frequency srake ingest --file archive.tar.gz \\ --checkpoint 5000 # Disable progress bar srake ingest --file archive.tar.gz \\ --no-progress # Set worker count srake ingest --file archive.tar.gz \\ --workers 8","pipeline-composition#Pipeline Composition":"# Commands accept stdin for easy chaining echo \"SRP123456\" | srake convert --to GSE cat accessions.txt | srake download --parallel 4 srake search \"RNA-Seq\" | srake download --type fastq","quality-filtering#Quality Filtering":"# High-quality data only srake ingest --file archive.tar.gz \\ --min-reads 10000000 \\ --min-bases 1000000000 Preview Mode: Use --stats-only to see what would be imported without actually inserting data","quick-start#Quick Start":"### Ingest SRA Metadata Let srake automatically select and download the appropriate archive: ```bash srake ingest --auto ``` The --auto flag intelligently selects between daily updates or full datasets based on your database state Alternative ingestion methods: DailyMonthlyLocal FileRemote URL # Latest daily update srake ingest --daily # Full monthly archive srake ingest --monthly # Local archive file srake ingest --file /path/to/archive.tar.gz # Direct from NCBI srake ingest --file https://ftp.ncbi.nlm.nih.gov/sra/reports/Metadata/archive.tar.gz ### Build Search Index Before searching, build the search index: ```bash # Build initial index srake search index --build # Verify index srake search index --stats ``` ### Search Your Data **Simple Search**: ```bash srake search \"homo sapiens\" ``` **Advanced Query Syntax**: ```bash # Boolean operators srake search \"organism:human AND library_strategy:RNA-Seq\" --advanced # Field-specific search srake search \"platform:ILLUMINA OR platform:PACBIO\" --advanced # Wildcards and ranges srake search \"RNA* AND spots:[1000000 TO *]\" --advanced ``` **Filtered Search**: ```bash srake search \"cancer\" \\ --organism \"homo sapiens\" \\ --platform ILLUMINA \\ --library-strategy RNA-Seq \\ --spots-min 10000000 ``` **Aggregation \u0026 Analytics**: ```bash # Count by organism srake search \"RNA-Seq\" --aggregate-by organism # Get total count srake search \"cancer\" --count-only # Show facets srake search \"human\" --facets ``` **Export Results**: ```bash # JSON format srake search \"RNA-Seq\" --format json --output results.json # CSV with specific fields srake search \"cancer\" --format csv --fields \"accession,organism,platform\" # Accession list for batch download srake search \"single cell\" --format accession | xargs srake download ``` ### Convert Accessions Convert between SRA, GEO, and BioProject identifiers: ```bash # SRA to GEO srake convert SRP123456 --to GSE # GEO to SRA srake convert GSM123456 --to SRX # Batch conversion srake convert SRP001 SRP002 --to GSE --format json ``` ### Query Relationships Navigate the relationships between SRA entities: ```bash # Get all runs for a study srake runs SRP123456 # Get samples for an experiment srake samples SRX123456 --detailed # Get parent study from any accession srake studies SRR123456 ``` ### Download Data Download SRA files from multiple sources: ```bash # Basic download srake download SRR123456 # Download from AWS srake download SRR123456 --source aws --threads 4 # Download all runs for a study srake download SRP123456 --type fastq --parallel 4 ``` ### Start API Server Launch the REST API server for programmatic access: ```bash # Start server on default port 8080 srake server # Custom port srake server --port 3000 ``` API Access: Query via curl \"http://localhost:8080/api/search?q=human\u0026limit=10\"","resume-capability#Resume Capability":"srake automatically tracks progress and can resume from interruptions:\n### Automatic Resume ```bash # If interrupted, run the same command again srake ingest --file archive.tar.gz # Output: # Previous ingestion found: # Progress: 45.3% complete # Resume? (y/n): y ``` ### Check Status ```bash srake ingest --status # Current Ingestion Status # ──────────────────────── # Progress: 67.8% complete # Records: 2,345,678 # ETA: 12 minutes ``` ### Force Restart ```bash # Start fresh (ignore existing progress) srake ingest --file archive.tar.gz --force ```","structured-output#Structured Output":"# Export in various formats for processing srake search \"human\" --format json | jq '.results[].accession' srake convert SRP123456 --to GSE --format csv \u003e results.csv See the Automation Guide for more advanced scripting examples."},"title":"Getting Started"},"/nishad/srake/docs/reference/cli/":{"data":{"automatic-expansion#Automatic Expansion":"The download command automatically expands:\nSRP → all runs in the study SRX → all runs in the experiment SRS → all runs for the sample","cli-reference#CLI Reference":"CLI ReferenceComplete reference for all srake commands and options.","commands#Commands":"","environment-variables#Environment Variables":"NO_COLOR - Disable colored output globally SRAKE_DB - Default database path AWS_REGION - Affects download source auto-selection GCP_PROJECT - Affects download source auto-selection","examples#Examples":"# Auto-ingest best file srake ingest --auto # Non-interactive ingest (no prompts) srake ingest --auto --yes # Ingest with filters srake ingest --auto --taxon-ids 9606 --platforms ILLUMINA --strategies RNA-Seq # List available files srake ingest --list # Debug mode to see detailed processing srake ingest --auto --debug","examples-1#Examples":"# Basic search srake search \"homo sapiens\" # Search with filters srake search mouse --platform ILLUMINA --limit 10 # Export results srake search human --format json --output results.json","examples-10#Examples":"# Show database statistics srake db info","examples-2#Examples":"# Convert SRA Project to GEO Series srake convert SRP123456 --to GSE # Convert multiple accessions srake convert SRP001 SRP002 SRP003 --to GSE # Batch conversion from file srake convert --batch accessions.txt --to SRX --output results.json # Convert from stdin (pipe-friendly) echo \"SRP123456\" | srake convert --to GSE cat accession_list.txt | srake convert --to GSM --format json # Preview conversion without executing srake convert SRP123456 --to GSE --dry-run # Debug mode to see conversion details srake convert SRP123456 --to GSE --debug","examples-3#Examples":"# Get runs for a study srake runs SRP123456 # Get detailed run information srake runs SRX123456 --detailed # Export as JSON srake runs SRP123456 --format json --output runs.json","examples-4#Examples":"# Get samples for a study srake samples SRP123456 # Get detailed sample information srake samples SRP123456 --detailed # Export as CSV srake samples SRX123456 --format csv --output samples.csv","examples-5#Examples":"# Get experiments for a study srake experiments SRP123456 # Get experiments for a sample srake experiments SRS123456 --detailed","examples-6#Examples":"# Get study from an experiment srake studies SRX123456 # Get study from a run with details srake studies SRR123456 --detailed","examples-7#Examples":"# Basic download srake download SRR123456 # Download from AWS with parallel transfers srake download SRR123456 --source aws --threads 4 # Download all runs for a study srake download SRP123456 --type fastq --output ./data/ # Batch download from file srake download --list runs.txt --parallel 4 # Download from stdin (pipe-friendly) echo \"SRR123456\" | srake download --type fastq srake runs SRP123456 | srake download --parallel 4 # High-speed Aspera transfer srake download SRR123456 --aspera # Dry run to preview downloads srake download SRP123456 --dry-run # Non-interactive download (no prompts) srake download SRP123456 --yes # Debug mode for troubleshooting srake download SRR123456 --debug","examples-8#Examples":"# Get metadata for an experiment srake metadata SRX123456 # Get multiple accessions as JSON srake metadata SRX123456 SRX123457 --format json # Select specific fields srake metadata SRR999999 --fields title,platform,strategy","examples-9#Examples":"# Start server on default port srake server # Custom port and host srake server --port 9090 --host 0.0.0.0 # Development mode with debug logging srake server --dev --log-level debug","exit-codes#Exit Codes":"0 - Success 1 - General error 2 - Command line usage error 130 - Interrupted (Ctrl+C)","filtering-flags#Filtering Flags":"--taxon-ids - Filter by taxonomy IDs (comma-separated) --exclude-taxon-ids - Exclude taxonomy IDs --date-from - Start date for filtering --date-to - End date for filtering --organisms - Filter by organism names --platforms - Filter by platforms (ILLUMINA, OXFORD_NANOPORE, etc.) --strategies - Filter by library strategies (RNA-Seq, WGS, etc.) --min-reads - Minimum read count filter --max-reads - Maximum read count filter --stats-only - Only show statistics without inserting data","flags#Flags":"--auto - Auto-select the best file from NCBI --daily - Ingest the latest daily update --monthly - Ingest the latest monthly dataset --file - Ingest specific file (local or NCBI) --list - List available files without ingesting --db - Database path (default: “./data/metadata.db”) --force - Force ingestion even if data exists --no-progress - Disable progress bar","flags-1#Flags":"-o, --organism - Filter by organism --platform - Filter by platform --strategy - Filter by library strategy -l, --limit - Maximum results (default: 100) -f, --format - Output format (table|json|csv|tsv) --output - Save results to file --no-header - Omit header in output","flags-2#Flags":"--to - Target accession type (required) Options: GSE, SRP, SRX, GSM, SRR, SRS, PRJNA, BIOSAMPLE -f, --format - Output format (table|json|yaml|csv|tsv) -o, --output - Save results to file --batch - Read accessions from file --dry-run - Preview conversions without executing","flags-3#Flags":"-d, --detailed - Include detailed information -f, --format - Output format (table|json|yaml|csv|tsv) -o, --output - Save results to file -l, --limit - Limit number of results --fields - Comma-separated list of fields","flags-4#Flags":"-d, --detailed - Include organism and taxonomy information -f, --format - Output format (table|json|yaml|csv|tsv) -o, --output - Save results to file -l, --limit - Limit number of results","flags-5#Flags":"-d, --detailed - Include platform and library information -f, --format - Output format (table|json|yaml|csv|tsv) -o, --output - Save results to file -l, --limit - Limit number of results","flags-6#Flags":"-d, --detailed - Include abstract and full metadata -f, --format - Output format (table|json|yaml|csv|tsv) -o, --output - Save results to file","flags-7#Flags":"-s, --source - Download source (auto|ftp|aws|gcp|ncbi) -t, --type - File type (sra|fastq|fasta) -o, --output - Output directory (default: “./”) --threads - Download threads per file (default: 1) -p, --parallel - Parallel downloads (default: 1) --aspera - Use Aspera for high-speed transfer -l, --list - File containing accessions --retry - Number of retry attempts (default: 3) --validate - Validate downloaded files (default: true) --dry-run - Show what would be downloaded","flags-8#Flags":"-f, --format - Output format (table|json|yaml) --fields - Comma-separated list of fields --expand - Expand nested structures","flags-9#Flags":"-p, --port - Port to listen on (default: 8080) --host - Host to bind to (default: localhost) --db - Database path --log-level - Log level (debug|info|warn|error) --dev - Enable development mode","global-flags#Global Flags":"These flags are available for all commands:\n--no-color - Disable colored output -v, --verbose - Enable verbose output -q, --quiet - Suppress non-error output -y, --yes - Assume yes to all prompts (non-interactive mode) --debug - Enable debug output for troubleshooting --help - Show help for any command","output-formats#Output Formats":"Most commands support multiple output formats:\ntable (default) - Human-readable table with colors json - JSON format for programmatic use yaml - YAML format csv - Comma-separated values tsv - Tab-separated values xml - XML format (convert command only)","srake-convert#\u003ccode\u003esrake convert\u003c/code\u003e":"Convert between different accession types (SRA, GEO, BioProject, BioSample).\nsrake convert [ ...] [flags]","srake-db#\u003ccode\u003esrake db\u003c/code\u003e":"Database management commands.\nsrake db [flags]","srake-download#\u003ccode\u003esrake download\u003c/code\u003e":"Download SRA data files from multiple sources.\nsrake download [ ...] [flags]","srake-experiments#\u003ccode\u003esrake experiments\u003c/code\u003e":"Get all experiments for a study or sample.\nsrake experiments [flags]","srake-ingest#\u003ccode\u003esrake ingest\u003c/code\u003e":"Ingest SRA metadata from NCBI or local archives.\nsrake ingest [flags]","srake-metadata#\u003ccode\u003esrake metadata\u003c/code\u003e":"Get detailed metadata for specific accessions.\nsrake metadata [accessions...] [flags]","srake-runs#\u003ccode\u003esrake runs\u003c/code\u003e":"Get all runs for a study, experiment, or sample.\nsrake runs [flags]","srake-samples#\u003ccode\u003esrake samples\u003c/code\u003e":"Get all samples for a study or experiment.\nsrake samples [flags]","srake-search#\u003ccode\u003esrake search\u003c/code\u003e":"Search SRA metadata for experiments matching your query.\nsrake search [flags]","srake-server#\u003ccode\u003esrake server\u003c/code\u003e":"Start the API server for programmatic access.\nsrake server [flags]","srake-studies#\u003ccode\u003esrake studies\u003c/code\u003e":"Get study information for any SRA accession.\nsrake studies [flags]","subcommands#Subcommands":"info - Show database statistics and information","supported-conversions#Supported Conversions":"From To Description SRP GSE, SRX, SRR, SRS, PRJNA Study to related accessions SRX GSM, SRP, SRR, SRS Experiment to related accessions SRR SRX, SRP, GSM Run to parent accessions SRS SRX, GSM, BIOSAMPLE Sample to related accessions GSE SRP, GSM GEO Series to SRA/samples GSM SRX, SRR, GSE GEO Sample to SRA/series PRJNA SRP BioProject to SRA Project SAMN SRS BioSample to SRA Sample"},"title":"CLI Reference"}}