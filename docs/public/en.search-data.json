{"/nishad/srake/about/":{"data":{"":"This is the about page."},"title":"About"},"/nishad/srake/docs/":{"data":{"":"Welcome to the srake documentation! This guide will help you get started with processing NCBI SRA metadata efficiently.","features#Features":"Performance: Efficient record processing Memory Management: Streaming architecture for large files Pipeline: HTTP ‚Üí Gzip ‚Üí Tar ‚Üí XML ‚Üí Database streaming Filtering: Filter by taxonomy, organism, platform, and more Resume Support: Recovery from interruptions Search: Full-text search with SQLite backend","key-features#Key Features":"Getting StartedInstall and run srake in minutes Filtering SystemProcess only the data you need Resume CapabilityHandle interruptions gracefully API ReferenceREST API and Go library","quick-example#Quick Example":"# Install srake go install github.com/nishad/srake/cmd/srake@latest # Ingest SRA metadata with filters srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --platforms ILLUMINA \\ --strategies RNA-Seq # Search the database srake search \"homo sapiens\" --limit 10 # Start API server srake server --port 8080","what-is-srake#What is srake?":"srake is a tool for processing and querying NCBI SRA (Sequence Read Archive) metadata. Built with a streaming architecture, srake can process large compressed archives without intermediate storage."},"title":"Docs"},"/nishad/srake/docs/automation/":{"data":{"automation--scripting-guide#Automation \u0026amp; Scripting Guide":"Automation \u0026 Scripting Guidesrake is designed to work seamlessly in automated workflows and scripts, following clig.dev best practices for command-line interfaces.","batch-processing#Batch Processing":"# Process accessions from a file while IFS= read -r accession; do srake convert \"$accession\" --to GSE --quiet done \u003c accessions.txt # Or use stdin directly cat accessions.txt | srake convert --to GSE --output results.json","best-practices#Best Practices":"Always use --yes in scripts to avoid hanging on prompts Use --quiet to suppress non-essential output in production scripts Enable --debug when developing to understand command behavior Test with --dry-run first before running destructive operations Check exit codes for proper error handling Use structured output formats (JSON/TSV) for reliable parsing Leverage stdin for composability with other Unix tools Set appropriate timeouts for network operations in CI/CD","chaining-commands#Chaining Commands":"# Find all RNA-Seq experiments and download them srake search \"RNA-Seq\" --format tsv | \\ cut -f1 | \\ srake download --type fastq --parallel 4 # Convert a list of accessions cat accessions.txt | srake convert --to GSE --format json \u003e converted.json # Process search results through multiple tools srake search \"homo sapiens\" --limit 1000 | \\ grep \"ILLUMINA\" | \\ cut -f1 | \\ srake metadata --format json","cicd-integration#CI/CD Integration":"","cron-jobs#Cron Jobs":"Example cron job for automated daily ingestion:\n# Daily SRA metadata update at 2 AM 0 2 * * * /usr/local/bin/srake ingest --daily --yes --quiet \u003e\u003e /var/log/srake.log 2\u003e\u00261 # Weekly full ingest on Sundays 0 3 * * 0 /usr/local/bin/srake ingest --auto --yes --force \u003e\u003e /var/log/srake.log 2\u003e\u00261","debugging-scripts#Debugging Scripts":"Use the --debug flag to troubleshoot issues:\n# Enable debug output for detailed logging srake download SRR123456 --debug 2\u003e debug.log # Combine with verbose for maximum information srake convert SRP123456 --to GSE --debug --verbose","docker-integration#Docker Integration":"Run srake in containerized environments:\n# Non-interactive Docker execution docker run -v $(pwd)/data:/data \\ srake-image \\ srake ingest --auto --yes --db /data/metadata.db # Pipe data into containerized srake cat accessions.txt | docker run -i srake-image \\ srake convert --to GSE --format json","dry-run-mode#Dry Run Mode":"Test your commands without making changes using --dry-run:\n# Preview what would be downloaded srake download SRP123456 --dry-run # Check conversions before executing echo -e \"SRP001\\nSRP002\\nSRP003\" | srake convert --to GSE --dry-run","environment-variables#Environment Variables":"srake respects standard environment variables:\n# Disable colored output export NO_COLOR=1 # Custom database location export SRAKE_DB=/custom/path/metadata.db # Run with environment overrides NO_COLOR=1 srake search \"mouse\" --format table","error-handling#Error Handling":"srake follows Unix conventions for exit codes:\n0: Success 1: General error 2: Command line usage error #!/bin/bash set -e # Exit on any error # Check if download succeeded if srake download SRR123456 --yes --quiet; then echo \"Download successful\" else echo \"Download failed with exit code $?\" exit 1 fi","github-actions-example#GitHub Actions Example":"name: Update SRA Metadata on: schedule: - cron: '0 0 * * *' # Daily at midnight workflow_dispatch: jobs: update: runs-on: ubuntu-latest steps: - uses: actions/checkout@v3 - name: Setup Go uses: actions/setup-go@v4 with: go-version: '1.21' - name: Install srake run: go install github.com/nishad/srake/cmd/srake@latest - name: Update metadata run: | srake ingest --auto --yes --quiet srake db info - name: Upload database uses: actions/upload-artifact@v3 with: name: sra-metadata path: ./data/metadata.db","gitlab-ci-example#GitLab CI Example":"update-sra-metadata: stage: data script: - srake ingest --auto --yes --quiet - srake search \"homo sapiens\" --limit 100 --format json \u003e latest_human.json artifacts: paths: - ./data/metadata.db - latest_human.json only: - schedules","logging#Logging":"Redirect output streams for logging:\n# Log errors only srake ingest --auto 2\u003e errors.log # Log everything srake ingest --auto --verbose \u003e output.log 2\u003e\u00261 # Separate stdout and stderr srake search \"human\" \u003e results.txt 2\u003e errors.log # Tee for both console and file srake ingest --auto 2\u003e\u00261 | tee -a srake.log This guide ensures your srake automation is robust, maintainable, and follows Unix philosophy for maximum interoperability.","non-interactive-mode#Non-Interactive Mode":"When running srake in scripts or CI/CD pipelines, use the --yes flag to automatically accept all prompts:\n#!/bin/bash # Automated daily ingest script srake ingest --auto --yes --quiet","output-formats-for-scripts#Output Formats for Scripts":"Use structured output formats for easier parsing:\n# JSON for complex processing srake search \"mouse\" --format json | jq '.[] | .accession' # TSV for simple column extraction srake search \"human\" --format tsv --no-header | awk '{print $1}' # CSV for spreadsheet tools srake convert SRP123456 --to GSE --format csv \u003e results.csv","parallel-processing#Parallel Processing":"Leverage GNU parallel for large-scale processing:\n# Download multiple accessions in parallel cat accessions.txt | parallel -j 4 srake download {} --yes --quiet # Convert accessions in parallel parallel -j 8 srake convert {} --to GSE ::: SRP001 SRP002 SRP003","pipeline-composition-with-stdin#Pipeline Composition with stdin":"srake commands support stdin input, making them perfect for Unix pipelines:","shell-functions#Shell Functions":"Create helpful shell functions for common tasks:\n# Add to ~/.bashrc or ~/.zshrc # Quick SRA to GEO conversion sra2geo() { echo \"$1\" | srake convert --to GSE --quiet | tail -1 } # Download helper with defaults sra_download() { srake download \"$@\" --type fastq --parallel 4 --yes } # Search and count results sra_count() { srake search \"$1\" --format tsv --no-header | wc -l } # Usage $ sra2geo SRP123456 GSE98765 $ sra_download SRR123456 SRR123457 # Downloads with optimized settings $ sra_count \"homo sapiens RNA-Seq\" 1234"},"title":"Automation \u0026 Scripting"},"/nishad/srake/docs/compatibility/":{"data":{"1-streaming-architecture#1. Streaming Architecture":"Process 14GB+ files with minimal RAM No need to extract archives to disk Zero-copy data transfer","2-checkpoint-system#2. Checkpoint System":"Resume from exact interruption point Track progress across sessions No duplicate processing","3-integrated-filtering#3. Integrated Filtering":"Filter during ingestion, not after Reduce database size Faster subsequent queries","4-unified-cli#4. Unified CLI":"Single tool for all operations Consistent command structure No language-specific setup","advanced-feature-mapping#Advanced Feature Mapping":"","api-endpoints#API Endpoints":"For tools that need programmatic access, srake provides REST API equivalents:\n# Start API server srake server --port 8080 # Query endpoints curl \"http://localhost:8080/api/search?q=cancer\" curl \"http://localhost:8080/api/metadata/SRP123456\" curl \"http://localhost:8080/api/convert?from=SRP123456\u0026to=GSE\"","batch-operations#Batch Operations":"Other tools often require scripting:\n# pysradb for acc in accession_list: db.sra_metadata(acc) srake provides native batch support:\n# Batch conversion srake convert --batch accessions.txt --to GSE # Batch download srake download --list runs.txt --parallel 4 # Batch metadata srake metadata SRX001 SRX002 SRX003 --format json","command-equivalents#Command Equivalents":"","compatibility-with-other-tools#Compatibility with Other Tools":"Compatibility with Other Toolssrake provides comprehensive functionality that matches and extends popular SRA metadata tools. This guide shows how srake commands map to equivalent operations in other tools.","conclusion#Conclusion":"srake combines the best features of existing tools while adding unique capabilities like streaming processing, checkpoint recovery, and integrated filtering. It provides a unified, efficient solution for SRA metadata management that scales from small queries to massive dataset processing.","feature-comparison-matrix#Feature Comparison Matrix":"Feature srake SRAdb ffq pysradb MetaSRA Local Database ‚úÖ ‚úÖ ‚ùå ‚úÖ ‚ùå Streaming Processing ‚úÖ ‚ùå ‚ùå ‚ùå ‚ùå Accession Conversion ‚úÖ ‚úÖ ‚úÖ ‚úÖ ‚úÖ Multi-source Download ‚úÖ ‚úÖ ‚úÖ ‚úÖ ‚ùå Relationship Queries ‚úÖ ‚úÖ ‚ùå ‚úÖ ‚ùå Batch Operations ‚úÖ ‚úÖ ‚úÖ ‚úÖ ‚ùå Resume Capability ‚úÖ ‚ùå ‚ùå ‚úÖ ‚ùå Filtering on Ingest ‚úÖ ‚ùå ‚ùå ‚ùå ‚ùå REST API ‚úÖ ‚ùå ‚ùå ‚ùå ‚úÖ Aspera Support ‚úÖ ‚úÖ ‚ùå ‚úÖ ‚ùå","ffq--srake#ffq ‚Üí srake":"MetadataDownloadMulti-DB ffq:\n# Get metadata for an accession ffq SRR123456 # Get metadata with specific depth ffq -l 2 GSE123456 # Save to JSON ffq -o metadata.json SRR123456 srake:\n# Get metadata for an accession srake metadata SRR123456 # Get related metadata srake studies SRR123456 --detailed # Save to JSON srake metadata SRR123456 --format json --output metadata.json ffq:\n# Get FTP links ffq --ftp SRR123456 # Get AWS links ffq --aws SRR123456 # Get GCP links ffq --gcp SRR123456 srake:\n# Download from FTP srake download SRR123456 --source ftp # Download from AWS srake download SRR123456 --source aws # Download from GCP srake download SRR123456 --source gcp ffq:\n# Query from multiple databases ffq GSE123456 # Queries GEO ffq SRR123456 # Queries SRA ffq ENCSR000EYA # Queries ENCODE srake:\n# Convert between databases srake convert GSE123456 --to SRP # GEO to SRA srake convert SRR123456 --to GSM # SRA to GEO # Direct metadata query srake metadata GSE123456 # Handles any accession type","filtering-capabilities#Filtering Capabilities":"Most tools require post-processing:\n# SRAdb - filter after retrieval data \u003c- getSRA(search_terms = \"*\") filtered \u003c- subset(data, organism == \"Homo sapiens\") srake filters during ingestion:\n# Filter at source - more efficient srake ingest --auto \\ --organisms \"homo sapiens\" \\ --platforms ILLUMINA \\ --strategies RNA-Seq \\ --min-reads 10000000","from-ffq#From ffq":"Metadata retrieval:\n# ffq focuses on links # srake provides full metadata srake metadata SRR123456 --detailed Download URLs:\n# ffq shows URLs # srake downloads directly srake download SRR123456 --dry-run # To see URLs srake download SRR123456 # To download","from-pysradb#From pysradb":"Python to CLI:\n# pysradb requires Python scripting # srake works from command line srake convert GSM123456 --to SRX DataFrame to formats:\n# pysradb returns DataFrames # srake supports multiple formats srake search \"query\" --format csv","from-sradb#From SRAdb":"Database setup:\n# SRAdb: Download SQLite file # srake: Ingest directly srake ingest --auto Query syntax:\n# SRAdb: SQL queries # srake: Simple CLI commands srake search \"your query\" Output formats:\n# Both support multiple formats srake search \"query\" --format json","migration-guide#Migration Guide":"","performance-comparison#Performance Comparison":"Operation srake SRAdb pysradb 14GB Archive Ingestion 15 min 45 min* 35 min* Memory Usage 200MB 8GB+ 4GB+ Resume Support ‚úÖ ‚ùå ‚ùå Concurrent Processing ‚úÖ ‚ùå Limited *Requires full extraction to disk first","pysradb--srake#pysradb ‚Üí srake":"MetadataDownloadConversionSearch pysradb:\n# Get metadata from pysradb import SRAweb db = SRAweb() df = db.sra_metadata('SRP123456') # Detailed metadata df = db.sra_metadata('SRP123456', detailed=True) srake:\n# Get metadata srake metadata SRP123456 # Detailed metadata srake metadata SRP123456 --detailed --format json pysradb:\n# Download SRA files db.download(df, protocol='fasp') # Download with filters db.download(df, filter_by_library_strategy='RNA-Seq') srake:\n# Download with Aspera srake download SRP123456 --aspera # Download with filters (filter during ingest) srake ingest --auto --strategies RNA-Seq srake download SRP123456 pysradb:\n# GSM to SRP srp = db.gsm_to_srp(['GSM123456']) # SRP to GSE gse = db.srp_to_gse(['SRP123456']) # SRX to SRR srr = db.srx_to_srr(['SRX123456']) srake:\n# GSM to SRP srake convert GSM123456 --to SRP # SRP to GSE srake convert SRP123456 --to GSE # SRX to SRR srake runs SRX123456 pysradb:\n# Search by study results = db.search_by_study_title('cancer') # Search experiments results = db.search_sra_studies('breast cancer', max_results=100) srake:\n# Search studies srake search \"cancer\" --limit 100 # Search with filters srake search \"breast cancer\" --organism \"homo sapiens\" --limit 100","resume-and-recovery#Resume and Recovery":"Other tools typically lack resume:\n# pysradb - no built-in resume # If interrupted, must restart from beginning srake has intelligent resume:\n# Automatic resume from interruption srake ingest --file large_archive.tar.gz # If interrupted, rerun same command to resume","sradb-r-package--srake#SRAdb (R Package) ‚Üí srake":"ConversionDownloadSearchMetadata SRAdb:\n# Convert SRP to GSE sraConvert(in_acc = \"SRP123456\", out_type = \"gse\") # Convert GSM to SRX sraConvert(in_acc = \"GSM123456\", out_type = \"srx\") srake:\n# Convert SRP to GSE srake convert SRP123456 --to GSE # Convert GSM to SRX srake convert GSM123456 --to SRX SRAdb:\n# Download SRA files getSRAfile(in_acc = \"SRR123456\", method = \"curl\") # Download FASTQ files getFASTQfile(in_acc = \"SRR123456\", srcType = \"ftp\") srake:\n# Download SRA files srake download SRR123456 # Download FASTQ files srake download SRR123456 --type fastq SRAdb:\n# Search metadata getSRA(search_terms = \"breast cancer\", out_types = c(\"study\", \"sample\", \"experiment\")) srake:\n# Search metadata srake search \"breast cancer\" --format json SRAdb:\n# Get SRA info getSRAinfo(in_acc = \"SRP123456\", sra_con = sra_con) srake:\n# Get metadata srake metadata SRP123456 --detailed","unique-srake-advantages#Unique srake Advantages":""},"title":"Tool Compatibility"},"/nishad/srake/docs/examples/":{"data":{"advanced-filtering#Advanced Filtering":"","automation-scripts#Automation Scripts":"","batch-operations#Batch Operations":"","building-a-local-index#Building a Local Index":"Create a searchable index of specific data types:\n# 1. Ingest filtered data srake ingest --auto \\ --organisms \"homo sapiens,mus musculus\" \\ --strategies \"RNA-Seq,ChIP-Seq,ATAC-Seq\" \\ --min-reads 10000000 \\ --date-from 2023-01-01 # 2. Export metadata for indexing srake search \"*\" --limit 0 --format json \u003e all_metadata.json # 3. Start API server for queries srake server --port 8080 \u0026 # 4. Query via API curl \"http://localhost:8080/api/search?q=transcription+factor\u0026limit=20\"","bulk-download-with-filtering#Bulk Download with Filtering":"Download only high-quality runs from multiple experiments:\n# Get runs with quality metrics srake runs SRP123456 --detailed --format json | \\ jq '.[] | select(.total_bases \u003e 10000000000) | .run_accession' \u003e \\ high_quality_runs.txt # Download filtered runs srake download --list high_quality_runs.txt \\ --source aws \\ --parallel 4 \\ --threads 2","composable-commands#Composable Commands":"srake commands can be chained together using Unix pipes:\n# Find experiments ‚Üí Get runs ‚Üí Download srake search \"CRISPR\" --format tsv --no-header | \\ cut -f1 | \\ xargs -I {} srake runs {} --format tsv --no-header | \\ cut -f1 | \\ srake download --type fastq # Convert accessions in bulk cat geo_accessions.txt | \\ srake convert --to SRP --format tsv --no-header | \\ cut -f2 \u003e sra_projects.txt # Chain multiple conversions echo \"GSE123456\" | \\ srake convert --to SRP | \\ grep SRP | \\ srake runs --format json","converting-a-list-of-accessions#Converting a List of Accessions":"Convert multiple accessions from a publication supplementary table:\n# Create accession list cat \u003e geo_accessions.txt \u003c\u003c EOF GSE111111 GSE222222 GSE333333 GSE444444 EOF # Batch convert to SRA projects srake convert --batch geo_accessions.txt \\ --to SRP \\ --format json \\ --output sra_projects.json # Extract just the SRP IDs cat sra_projects.json | jq -r '.[] | select(.error == null) | .targets[]' \u003e srp_list.txt","creating-a-download-queue#Creating a Download Queue":"Generate and process a download queue:\n#!/bin/bash # download_queue.sh # Get all RNA-Seq runs from 2024 srake search \"RNA-Seq\" \\ --format json \\ --date-from 2024-01-01 | \\ jq -r '.results[].accession' \u003e rna_seq_2024.txt # Process in batches of 10 split -l 10 rna_seq_2024.txt batch_ # Download each batch for batch in batch_*; do echo \"Processing $batch\" srake download --list $batch \\ --parallel 2 \\ --output ./downloads/ sleep 60 # Pause between batches done","cross-referencing-multiple-studies#Cross-referencing Multiple Studies":"Compare samples across different studies:\n# Get samples from multiple studies srake samples SRP001 --format json \u003e study1_samples.json srake samples SRP002 --format json \u003e study2_samples.json srake samples SRP003 --format json \u003e study3_samples.json # Or in batch for study in SRP001 SRP002 SRP003; do srake samples $study --detailed --format json \u003e ${study}_samples.json done","daily-report-generator#Daily Report Generator":"#!/bin/bash # daily_report.sh DATE=$(date +%Y-%m-%d) REPORT=\"report_$DATE.html\" cat \u003e $REPORT \u003c\u003c EOF SRA Daily Report - $DATE SRA Daily Report Generated: $(date)\nEOF # Database statistics echo \"Database Statistics\" \u003e\u003e $REPORT srake db info \u003e\u003e $REPORT echo \"\" \u003e\u003e $REPORT # New studies today echo \"New Studies\" \u003e\u003e $REPORT srake search \"*\" --date-from $DATE --format table \u003e\u003e $REPORT echo \"\" \u003e\u003e $REPORT echo \"\" \u003e\u003e $REPORT # Email report mail -s \"SRA Daily Report $DATE\" \\ -a $REPORT \\ team@example.com \u003c /dev/null","data-discovery#Data Discovery":"","debugging-failed-downloads#Debugging Failed Downloads":"# Dry run to check URLs srake download SRR123456 --dry-run --verbose # Test different sources for source in ftp aws gcp ncbi; do echo \"Testing $source\" srake download SRR123456 \\ --source $source \\ --dry-run done # Use verbose mode for debugging srake download SRR123456 \\ --verbose \\ --retry 5","downloading-data-for-a-published-study#Downloading Data for a Published Study":"Download all FASTQ files for a study mentioned in a paper:\n# Convert GEO accession from paper to SRA srake convert GSE123456 --to SRP # Get all runs for the study srake runs SRP123456 --format json --output runs.json # Download all runs in parallel srake download SRP123456 \\ --type fastq \\ --source aws \\ --parallel 4 \\ --output ./fastq_files/","examples-and-use-cases#Examples and Use Cases":"Examples and Use CasesReal-world examples demonstrating common workflows with srake, including pipeline composition and automation patterns.","exploring-platform-specific-data#Exploring Platform-Specific Data":"Find all Oxford Nanopore sequencing data:\n# Ingest only Nanopore data srake ingest --auto \\ --platforms OXFORD_NANOPORE # Search for specific applications srake search \"metagenome\" \\ --platform OXFORD_NANOPORE \\ --limit 50","finding-related-experiments#Finding Related Experiments":"Discover all experiments related to a sample:\n# Start with a sample accession SAMPLE=\"SRS123456\" # Get all experiments for this sample srake experiments $SAMPLE --detailed # Get the parent study srake studies $SAMPLE # Get all other samples in the study STUDY=$(srake studies $SAMPLE --format json | jq -r '.[0].study_accession') srake samples $STUDY --detailed","finding-rna-seq-data-for-a-specific-organism#Finding RNA-Seq Data for a Specific Organism":"Find all human RNA-Seq experiments published in 2024:\n# Ingest with filters srake ingest --auto \\ --taxon-ids 9606 \\ --strategies RNA-Seq \\ --date-from 2024-01-01 # Search for specific conditions srake search \"breast cancer\" \\ --organism \"homo sapiens\" \\ --strategy RNA-Seq \\ --limit 100 \\ --format csv \\ --output breast_cancer_studies.csv","handling-large-result-sets#Handling Large Result Sets":"# Paginate through large results OFFSET=0 LIMIT=1000 while true; do COUNT=$(srake search \"human\" \\ --offset $OFFSET \\ --limit $LIMIT \\ --format json | \\ jq '.results | length') if [ $COUNT -eq 0 ]; then break fi # Process batch srake search \"human\" \\ --offset $OFFSET \\ --limit $LIMIT \\ --format json \u003e batch_$OFFSET.json OFFSET=$((OFFSET + LIMIT)) done","incremental-updates#Incremental Updates":"Keep your database current with minimal overhead:\n#!/bin/bash # daily_update.sh # Check last update LAST_UPDATE=$(srake db info | grep \"Last update\" | cut -d: -f2) # Ingest only new data srake ingest --daily \\ --date-from \"$LAST_UPDATE\" \\ --no-progress # Log update echo \"$(date): Updated from $LAST_UPDATE\" \u003e\u003e update.log","integration-examples#Integration Examples":"","metadata-analysis-pipeline#Metadata Analysis Pipeline":"Extract and analyze metadata for a research domain:\n# Get all cancer-related studies srake search \"cancer\" --format json --limit 1000 \u003e cancer_studies.json # Extract platform distribution cat cancer_studies.json | \\ jq -r '.results[].platform' | \\ sort | uniq -c | sort -rn # Get temporal distribution cat cancer_studies.json | \\ jq -r '.results[].published' | \\ cut -d'-' -f1 | \\ sort | uniq -c","multi-criteria-filtering#Multi-criteria Filtering":"Complex filtering for specific research needs:\n# Ingest single-cell RNA-seq from human brain srake ingest --auto \\ --taxon-ids 9606 \\ --organisms \"homo sapiens\" \\ --strategies \"RNA-Seq\" \\ --min-reads 100000000 \\ --filter-verbose # Search with additional criteria srake search \"brain OR neuron OR glia\" \\ --format json | \\ jq '.results[] | select(.title | test(\"single.cell|sc.?RNA|10x\"; \"i\"))'","next-steps#Next Steps":"Review the CLI Reference for detailed command options Explore the API Documentation for programmatic access Check Performance Tips for optimization strategies","parallel-processing#Parallel Processing":"Maximize throughput with parallel operations:\n# Parallel conversion cat accessions.txt | \\ parallel -j 4 'srake convert {} --to GSE --format json \u003e {}.json' # Parallel metadata fetch cat studies.txt | \\ parallel -j 8 'srake metadata {} --format json' \u003e all_metadata.jsonl # Parallel download with resource limits nice -n 10 srake download --list large_dataset.txt \\ --parallel 4 \\ --threads 2 \\ --output /data/downloads/","performance-optimization#Performance Optimization":"","quality-control-pipeline#Quality Control Pipeline":"Filter and validate high-quality datasets:\n# Function to check data quality check_quality() { local accession=$1 # Get run information srake runs $accession --format json | \\ jq -r '.[] | \"\\(.run_accession):\\(.total_bases):\\(.total_spots)\"' | \\ while IFS=: read -r run bases spots; do if [ $bases -gt 10000000000 ]; then echo \"$run PASS\" else echo \"$run FAIL\" fi done } # Check multiple studies for study in SRP001 SRP002 SRP003; do echo \"Checking $study\" check_quality $study done","research-workflows#Research Workflows":"","stream-processing#Stream Processing":"Process large datasets without intermediate files:\n# Real-time filtering and conversion srake search \"RNA-Seq\" --format tsv --no-header | \\ awk '$3 \u003e 1000000' | \\ cut -f1 | \\ while read acc; do srake convert $acc --to GSE --quiet done # Parallel processing with xargs srake search \"mouse\" --limit 100 --format tsv --no-header | \\ cut -f1 | \\ xargs -P 4 -I {} srake metadata {} --format json --quiet","troubleshooting-examples#Troubleshooting Examples":"","unix-pipeline-integration#Unix Pipeline Integration":""},"title":"Examples"},"/nishad/srake/docs/features/":{"data":{"":"","architecture#Architecture":"Explore the powerful features that make srake the fastest and most efficient SRA metadata processor.\nCore Features Filtering System Process only the data you need with smart filters for taxonomy, organism, platform, date ranges, and quality metrics.\nResume Capability Handle interruptions gracefully with automatic checkpoint system and progress tracking.\nPerformance Optimizations that enable processing 20,000+ records per second with minimal memory usage.\nArchitecture Zero-copy streaming design that processes multi-gigabyte files without intermediate storage.","core-features#Core Features":"","filtering-system#Filtering System":"","performance#Performance":"","resume-capability#Resume Capability":""},"title":"Features"},"/nishad/srake/docs/features/filtering/":{"data":{"":"The filtering system allows you to process only the data you need, reducing storage requirements.","best-practices#Best Practices":"","common-patterns#Common Patterns":"Model OrganismsFocus on well-studied species for comparative analysis Recent DataFilter by date for the latest sequencing technologies High QualityUse quality filters for publication-ready datasets Technology SpecificFilter by platform for consistent processing pipelines","complex-filter-combinations#Complex Filter Combinations":"","date-range-filtering#Date Range Filtering":"Filter by submission or publication dates:\nYear 2024Last 90 DaysHistorical Data srake ingest --file archive.tar.gz \\ --date-from 2024-01-01 \\ --date-to 2024-12-31 srake ingest --file archive.tar.gz \\ --date-from 2024-10-01 srake ingest --file archive.tar.gz \\ --date-to 2020-12-31","filter-configuration-files#Filter Configuration Files":"For complex, reusable filter sets, use YAML configuration:\nconfig.yamlUsage # filters.yaml taxonomy: include: [9606, 10090] exclude: [562] platforms: - ILLUMINA - OXFORD_NANOPORE strategies: - RNA-Seq - WGS date: from: \"2024-01-01\" to: \"2024-12-31\" quality: min_reads: 10000000 min_bases: 1000000000 # Use configuration file srake ingest --file archive.tar.gz \\ --filter-config filters.yaml # Override specific settings srake ingest --file archive.tar.gz \\ --filter-config filters.yaml \\ --date-from 2025-01-01","filter-processing#Filter Processing":"Filters are applied during the streaming pipeline, allowing efficient processing of large datasets without loading everything into memory.","filter-types#Filter Types":"","filters-not-working#Filters Not Working?":"### Verify Filter Syntax ```bash # Check your command srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --verbose ``` ### Check Statistics ```bash # Use stats-only mode srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --stats-only ``` ### Enable Verbose Output ```bash # See detailed filtering decisions srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --verbose --log-level debug ```","library-strategy-filtering#Library Strategy Filtering":"Filter by experimental strategy:\nRNA-SeqGenomicsEpigenomicsMultiple # RNA sequencing only srake ingest --file archive.tar.gz \\ --strategies RNA-Seq # Whole genome sequencing srake ingest --file archive.tar.gz \\ --strategies WGS,WXS # Epigenomics studies srake ingest --file archive.tar.gz \\ --strategies ChIP-Seq,ATAC-Seq,Bisulfite-Seq # Multiple strategies srake ingest --file archive.tar.gz \\ --strategies RNA-Seq,WGS,ChIP-Seq Common Strategies:\nRNA-Seq - RNA sequencing WGS - Whole Genome Sequencing WXS - Whole Exome Sequencing ChIP-Seq - Chromatin IP ATAC-Seq - Chromatin accessibility Bisulfite-Seq - DNA methylation Hi-C - Chromosome conformation","next-steps#Next Steps":"Resume CapabilityHandle interruptions gracefully PerformanceOptimization techniques ExamplesReal-world filtering scenarios","organism-name-filtering#Organism Name Filtering":"Filter by scientific names when you don‚Äôt know the taxonomy IDs:\n# Single organism srake ingest --file archive.tar.gz \\ --organisms \"homo sapiens\" # Multiple organisms srake ingest --file archive.tar.gz \\ --organisms \"homo sapiens,mus musculus,rattus norvegicus\"","overview#Overview":"The filtering system operates during the streaming pipeline, applying filters before data is inserted into the database:\nKey features:\nMemory Efficient: Filters applied during streaming Early Rejection: Unwanted records discarded before database operations Statistics: Track filtering effectiveness","platform-filtering#Platform Filtering":"IlluminaLong ReadsMultipleAll Platforms # Illumina data only srake ingest --file archive.tar.gz \\ --platforms ILLUMINA # Long-read platforms srake ingest --file archive.tar.gz \\ --platforms OXFORD_NANOPORE,PACBIO_SMRT # Multiple platforms srake ingest --file archive.tar.gz \\ --platforms ILLUMINA,ION_TORRENT Available Platforms:\nILLUMINA OXFORD_NANOPORE PACBIO_SMRT ION_TORRENT LS454 ABI_SOLID COMPLETE_GENOMICS","preview-mode#Preview Mode":"Test your filters without inserting data:\n### Run with --stats-only ```bash srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --platforms ILLUMINA \\ --strategies RNA-Seq \\ --stats-only ``` ### Review Statistics ``` Filter Statistics ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Total XML files: 150,234 Files matching filters: 12,456 (8.3%) Records that would be inserted: Studies: 3,234 Experiments: 12,456 Samples: 11,234 Runs: 15,678 Estimated database size: 1.2 GB Processing time estimate: 15 minutes ``` ### Adjust and Apply ```bash # Apply the filters for real srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --platforms ILLUMINA \\ --strategies RNA-Seq ```","quality-filtering#Quality Filtering":"Filter by sequencing depth and quality:\nHigh CoverageSpecific RangeUltra Deep # Minimum 10M reads and 1GB bases srake ingest --file archive.tar.gz \\ --min-reads 10000000 \\ --min-bases 1000000000 # Between 5M and 50M reads srake ingest --file archive.tar.gz \\ --min-reads 5000000 \\ --max-reads 50000000 # Ultra-deep sequencing (10GB+ bases) srake ingest --file archive.tar.gz \\ --min-bases 10000000000","research-specific-workflows#Research-Specific Workflows":"Cancer ResearchPopulation GeneticsMicrobiomeSingle Cell # Human cancer RNA-Seq studies srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --strategies RNA-Seq,WGS \\ --date-from 2023-01-01 \\ --min-reads 20000000 # Population genomics data srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --strategies WGS \\ --platforms ILLUMINA \\ --min-bases 30000000000 # Microbiome studies srake ingest --file archive.tar.gz \\ --strategies AMPLICON,WGS \\ --platforms ILLUMINA,ION_TORRENT \\ --exclude-taxon-ids 9606,10090 # Single-cell RNA-Seq srake ingest --file archive.tar.gz \\ --taxon-ids 9606,10090 \\ --strategies RNA-Seq \\ --date-from 2022-01-01 \\ --platforms ILLUMINA","taxonomy-filtering#Taxonomy Filtering":"Filter by NCBI taxonomy IDs to focus on specific organisms:\nSingle SpeciesMultiple SpeciesExclude TaxaCombined # Human data only (taxonomy ID 9606) srake ingest --file archive.tar.gz \\ --taxon-ids 9606 # Human, mouse, and zebrafish srake ingest --file archive.tar.gz \\ --taxon-ids 9606,10090,7955 # Exclude viruses and bacteria srake ingest --file archive.tar.gz \\ --exclude-taxon-ids 32630,2697049,562 # Mammals excluding E. coli contamination srake ingest --file archive.tar.gz \\ --taxon-ids 9606,10090 \\ --exclude-taxon-ids 562 Common Taxonomy IDs:\n9606 - Homo sapiens (human) 10090 - Mus musculus (mouse) 7955 - Danio rerio (zebrafish) 7227 - Drosophila melanogaster 562 - Escherichia coli","tips-for-effective-filtering#Tips for Effective Filtering":"Start with ‚Äìstats-only to preview results Use taxonomy filters for targeted datasets Combine filters for precise data selection Save configurations for reproducible workflows Monitor statistics to verify filter effectiveness","troubleshooting#Troubleshooting":""},"title":"Filtering System"},"/nishad/srake/docs/features/resume/":{"data":{"":"srake includes intelligent resume functionality that handles interruptions gracefully, allowing you to continue processing from where you left off.","architecture#Architecture":"","automatic-resume#Automatic Resume":"Simply run the same command again after interruption:\n# Original command srake ingest --file NCBI_SRA_Full_20250818.tar.gz # If interrupted, run again srake ingest --file NCBI_SRA_Full_20250818.tar.gz # Output: Previous ingestion found: Source: NCBI_SRA_Full_20250818.tar.gz Progress: 45.3% complete (6.3GB/14GB) Records: 1,234,567 processed Started: 2025-01-17 10:30:00 Resume from last position? (y/n): y Resuming from: experiment_batch_042.xml [====================\u003e.................] 45.3% | 6.3GB/14GB | ETA: 15 min","batch-processing#Batch Processing":"# Process multiple archives with resume support for archive in *.tar.gz; do srake ingest --file \"$archive\" # Each file has independent resume tracking done","best-practices#Best Practices":"Let It Resume: Don‚Äôt use --force unless necessary Regular Checkpoints: Default (1000 records) works well for most cases Monitor Progress: Use --verbose to see detailed progress Keep Database: Don‚Äôt delete metadata.db during processing Network Stability: For large files, ensure stable connection","check-status#Check Status":"View current or last ingestion status:\nsrake ingest --status # Output: Current Ingestion Status ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ Source: NCBI_SRA_Full_20250818.tar.gz State: In Progress Progress: 67.8% complete Records Processed: 2,345,678 Start Time: 2025-01-17 10:30:00 Last Update: 2025-01-17 11:45:23 Estimated Time Remaining: 12 minutes","cleanup-policy#Cleanup Policy":"Progress records kept for 30 days after completion Failed attempts kept for 7 days Manual cleanup available via --cleanup","configure-checkpoints#Configure Checkpoints":"Adjust checkpoint frequency for your needs:\n# Checkpoint every 5000 records (less frequent) srake ingest --file archive.tar.gz --checkpoint 5000 # Checkpoint every 100 records (more frequent, safer) srake ingest --file archive.tar.gz --checkpoint 100","corrupted-progress#Corrupted Progress":"If progress records are corrupted:\n# Clean up progress records srake ingest --cleanup # Start fresh srake ingest --file archive.tar.gz","examples#Examples":"","file-identification#File Identification":"Files are identified by:\nSource URL/path SHA-256 hash of first 1MB File size Modification time","filtered-resume#Filtered Resume":"# Complex filter with resume srake ingest --file huge_archive.tar.gz \\ --taxon-ids 9606,10090 \\ --platforms ILLUMINA \\ --strategies RNA-Seq,WGS \\ --date-from 2024-01-01 \\ --min-reads 10000000 # Power failure at 60% # Resume with exact same filters srake ingest --file huge_archive.tar.gz \\ --taxon-ids 9606,10090 \\ --platforms ILLUMINA \\ --strategies RNA-Seq,WGS \\ --date-from 2024-01-01 \\ --min-reads 10000000 # Continues from 60% with filters intact","force-fresh-start#Force Fresh Start":"Override existing progress and start from beginning:\nsrake ingest --file archive.tar.gz --force # Warning shown: ‚ö†Ô∏è Existing progress will be discarded Continue? (y/n): y Starting fresh ingestion...","how-it-works#How It Works":"","interactive-mode#Interactive Mode":"Get prompted before resuming:\nsrake ingest --file archive.tar.gz --interactive # Always prompts: Previous ingestion found. Resume? (y/n):","key-features#Key Features":"Automatic Progress Tracking: Real-time tracking of download and processing progress Database-Backed Persistence: Progress stored in SQLite for reliability Checkpoint System: Periodic checkpoints for accurate recovery points File-Level Deduplication: Skip already-processed XML files on resume HTTP Range Support: Resume downloads from exact byte position Smart Recovery: Automatically detect and resume interrupted sessions","network-issues#Network Issues":"For unstable networks:\n# Increase retry attempts srake ingest --file https://ftp.ncbi.nlm.nih.gov/sra/archive.tar.gz \\ --max-retries 10 \\ --retry-delay 30","next-steps#Next Steps":"Learn about Performance Optimizations Explore Architecture Details See Real-World Examples","overview#Overview":"Processing large SRA metadata archives (14GB+) can take significant time. Network issues, system crashes, or user interruptions can occur during processing. srake‚Äôs resume capability ensures you never have to start over from the beginning.","performance-impact#Performance Impact":"Resume capability adds minimal overhead:\nAspect Impact Memory Usage \u003c 1MB for progress tracking Processing Speed \u003c 5% reduction Checkpoint Time \u003c 100ms per checkpoint Resume Time \u003c 5 seconds to restart Database Size \u003c 100KB for progress records","progress-database-schema#Progress Database Schema":"CREATE TABLE ingest_progress ( id INTEGER PRIMARY KEY, source_url TEXT NOT NULL, source_hash TEXT UNIQUE NOT NULL, total_bytes INTEGER, downloaded_bytes INTEGER, processed_bytes INTEGER, last_tar_position INTEGER, last_xml_file TEXT, records_processed INTEGER, state TEXT, started_at TIMESTAMP, updated_at TIMESTAMP, completed_at TIMESTAMP, error_message TEXT ); CREATE TABLE processed_files ( id INTEGER PRIMARY KEY, progress_id INTEGER, filename TEXT NOT NULL, processed_at TIMESTAMP, FOREIGN KEY (progress_id) REFERENCES ingest_progress(id) );","progress-tracking#Progress Tracking":"srake tracks multiple aspects of progress:\nDownload Progress\nBytes downloaded vs. total size HTTP range support for partial downloads Network failure recovery Processing Progress\nCurrent tar position in archive Last processed XML file Records inserted into database Checkpoint System\nPeriodic checkpoints (default: every 1000 records) Safe points for recovery Minimal performance impact (\u003c 100ms)","recovery-process#Recovery Process":"Validation Phase\nVerify source file exists/accessible Check source hash matches Validate database consistency Restoration Phase\nSeek to last tar position Skip processed XML files Restore counters and statistics Continuation Phase\nResume normal processing Continue checkpoint creation Update progress tracking","research-workflow#Research Workflow":"# Start large ingestion Friday evening srake ingest --monthly \\ --taxon-ids 9606 \\ --strategies RNA-Seq # System maintenance interrupts at 30% # Resume Monday morning srake ingest --monthly \\ --taxon-ids 9606 \\ --strategies RNA-Seq # Continues from 30%, completes successfully","resume-detection#Resume Detection":"When you run an ingest command, srake automatically:\nChecks for existing progress records Validates the source matches Offers to resume or start fresh Resumes from the last safe checkpoint","resume-not-working#Resume Not Working":"If resume doesn‚Äôt work as expected:\nCheck source file hasn‚Äôt changed\n# View progress details srake ingest --status Verify database integrity\n# Database location ls -la ./data/metadata.db Force restart if needed\nsrake ingest --file archive.tar.gz --force","resume-with-filters#Resume with Filters":"Resume works seamlessly with filtering:\n# Original filtered ingestion srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --platforms ILLUMINA # Resume with same filters applied srake ingest --file archive.tar.gz \\ --taxon-ids 9606 \\ --platforms ILLUMINA # Filters are preserved and reapplied","state-management#State Management":"Progress states:\npending: Initialized but not started downloading: Downloading from remote source processing: Processing tar archive completed: Successfully finished failed: Error occurred cancelled: User cancelled","technical-details#Technical Details":"","troubleshooting#Troubleshooting":"","using-resume#Using Resume":""},"title":"Resume Capability"},"/nishad/srake/docs/first-page/":{"data":{"":"A simple demo page."},"title":"Demo Page"},"/nishad/srake/docs/folder/":{"data":{"":"Pages can be organized into folders."},"title":"Folder"},"/nishad/srake/docs/folder/leaf/":{"data":{"":"This page is under a folder."},"title":"Leaf Page"},"/nishad/srake/docs/getting-started/":{"data":{"":"","automation-features#Automation Features":"srake follows clig.dev best practices for CLI design, making it perfect for automation:","configuration-options#Configuration Options":"","database-management#Database Management":"Database Info:\nsrake db info # Shows: # ‚Ä¢ Database size # ‚Ä¢ Table counts # ‚Ä¢ Index status Custom Location:\nsrake ingest \\ --file archive.tar.gz \\ --db /custom/path/db.sqlite Verbose Mode:\nsrake ingest \\ --file archive.tar.gz \\ --verbose","dry-run--debug#Dry Run \u0026amp; Debug":"# Preview actions without executing srake download SRP123456 --dry-run # Enable debug output for troubleshooting srake convert SRP123456 --to GSE --debug","filter-by-date-range#Filter by Date Range":"# Data from 2024 only srake ingest --file archive.tar.gz \\ --date-from 2024-01-01 \\ --date-to 2024-12-31","filter-by-platform--strategy#Filter by Platform \u0026amp; Strategy":"Illumina RNA-Seq:\nsrake ingest --file archive.tar.gz \\ --platforms ILLUMINA \\ --strategies RNA-Seq Oxford Nanopore WGS:\nsrake ingest --file archive.tar.gz \\ --platforms OXFORD_NANOPORE \\ --strategies WGS","filter-by-taxonomy#Filter by Taxonomy":"Single SpeciesMultiple SpeciesExclude Species # Human data only (taxonomy ID 9606) srake ingest --file archive.tar.gz \\ --taxon-ids 9606 # Human, mouse, and zebrafish srake ingest --file archive.tar.gz \\ --taxon-ids 9606,10090,7955 # Exclude viruses srake ingest --file archive.tar.gz \\ --exclude-taxon-ids 32630,2697049","filtering-options#Filtering Options":"Filtering helps reduce database size by processing only the data you need.","getting-help#Getting Help":"Need assistance? Check these resources:\nüìö FAQ üí¨ GitHub Discussions üêõ Report Issues üìñ Full Documentation","installation#Installation":"srake provides multiple installation methods to suit your needs:\nGoDockerBinarySource Requirements: Go 1.19 or later\ngo install github.com/nishad/srake/cmd/srake@latest Verify the installation:\nsrake --version Run in container:\n# Pull the image docker pull ghcr.io/nishad/srake:latest # Run with volume mount docker run -v $(pwd)/data:/data \\ ghcr.io/nishad/srake:latest \\ ingest --auto Download pre-built binaries:\n# Linux/macOS wget https://github.com/nishad/srake/releases/latest/download/srake-$(uname -s)-$(uname -m).tar.gz tar -xzf srake-*.tar.gz sudo mv srake /usr/local/bin/ Verify installation:\nsrake --version Build from source:\ngit clone https://github.com/nishad/srake.git cd srake go build -o srake ./cmd/srake ./srake --help","next-steps#Next Steps":"Filtering SystemLearn advanced filtering techniques Resume CapabilityHandle large files reliably API ReferenceProgrammatic access guide ExamplesReal-world use cases","non-interactive-mode#Non-Interactive Mode":"# Use --yes flag to skip all prompts srake ingest --auto --yes srake download SRP123456 --yes","performance-tuning#Performance Tuning":"CheckpointsProgressConcurrency # Adjust checkpoint frequency srake ingest --file archive.tar.gz \\ --checkpoint 5000 # Disable progress bar srake ingest --file archive.tar.gz \\ --no-progress # Set worker count srake ingest --file archive.tar.gz \\ --workers 8","pipeline-composition#Pipeline Composition":"# Commands accept stdin for easy chaining echo \"SRP123456\" | srake convert --to GSE cat accessions.txt | srake download --parallel 4 srake search \"RNA-Seq\" | srake download --type fastq","quality-filtering#Quality Filtering":"# High-quality data only srake ingest --file archive.tar.gz \\ --min-reads 10000000 \\ --min-bases 1000000000 Preview Mode: Use --stats-only to see what would be imported without actually inserting data","quick-start#Quick Start":"### Ingest SRA Metadata Let srake automatically select and download the appropriate archive: ```bash srake ingest --auto ``` The --auto flag intelligently selects between daily updates or full datasets based on your database state Alternative ingestion methods: DailyMonthlyLocal FileRemote URL # Latest daily update srake ingest --daily # Full monthly archive srake ingest --monthly # Local archive file srake ingest --file /path/to/archive.tar.gz # Direct from NCBI srake ingest --file https://ftp.ncbi.nlm.nih.gov/sra/reports/Metadata/archive.tar.gz ### Search Your Data **Simple Search**: ```bash srake search \"homo sapiens\" ``` **Filtered Search**: ```bash srake search \"cancer\" \\ --organism \"homo sapiens\" \\ --platform ILLUMINA ``` **Export Results**: ```bash srake search \"RNA-Seq\" \\ --format json \\ --output results.json ``` ### Convert Accessions Convert between SRA, GEO, and BioProject identifiers: ```bash # SRA to GEO srake convert SRP123456 --to GSE # GEO to SRA srake convert GSM123456 --to SRX # Batch conversion srake convert SRP001 SRP002 --to GSE --format json ``` ### Query Relationships Navigate the relationships between SRA entities: ```bash # Get all runs for a study srake runs SRP123456 # Get samples for an experiment srake samples SRX123456 --detailed # Get parent study from any accession srake studies SRR123456 ``` ### Download Data Download SRA files from multiple sources: ```bash # Basic download srake download SRR123456 # Download from AWS srake download SRR123456 --source aws --threads 4 # Download all runs for a study srake download SRP123456 --type fastq --parallel 4 ``` ### Start API Server Launch the REST API server for programmatic access: ```bash # Start server on default port 8080 srake server # Custom port srake server --port 3000 ``` API Access: Query via curl \"http://localhost:8080/api/search?q=human\u0026limit=10\"","resume-capability#Resume Capability":"srake automatically tracks progress and can resume from interruptions:\n### Automatic Resume ```bash # If interrupted, run the same command again srake ingest --file archive.tar.gz # Output: # Previous ingestion found: # Progress: 45.3% complete # Resume? (y/n): y ``` ### Check Status ```bash srake ingest --status # Current Ingestion Status # ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ # Progress: 67.8% complete # Records: 2,345,678 # ETA: 12 minutes ``` ### Force Restart ```bash # Start fresh (ignore existing progress) srake ingest --file archive.tar.gz --force ```","structured-output#Structured Output":"# Export in various formats for processing srake search \"human\" --format json | jq '.results[].accession' srake convert SRP123456 --to GSE --format csv \u003e results.csv See the Automation Guide for more advanced scripting examples."},"title":"Getting Started"},"/nishad/srake/docs/reference/cli/":{"data":{"automatic-expansion#Automatic Expansion":"The download command automatically expands:\nSRP ‚Üí all runs in the study SRX ‚Üí all runs in the experiment SRS ‚Üí all runs for the sample","cli-reference#CLI Reference":"CLI ReferenceComplete reference for all srake commands and options.","commands#Commands":"","environment-variables#Environment Variables":"NO_COLOR - Disable colored output globally SRAKE_DB - Default database path AWS_REGION - Affects download source auto-selection GCP_PROJECT - Affects download source auto-selection","examples#Examples":"# Auto-ingest best file srake ingest --auto # Non-interactive ingest (no prompts) srake ingest --auto --yes # Ingest with filters srake ingest --auto --taxon-ids 9606 --platforms ILLUMINA --strategies RNA-Seq # List available files srake ingest --list # Debug mode to see detailed processing srake ingest --auto --debug","examples-1#Examples":"# Basic search srake search \"homo sapiens\" # Search with filters srake search mouse --platform ILLUMINA --limit 10 # Export results srake search human --format json --output results.json","examples-10#Examples":"# Show database statistics srake db info","examples-2#Examples":"# Convert SRA Project to GEO Series srake convert SRP123456 --to GSE # Convert multiple accessions srake convert SRP001 SRP002 SRP003 --to GSE # Batch conversion from file srake convert --batch accessions.txt --to SRX --output results.json # Convert from stdin (pipe-friendly) echo \"SRP123456\" | srake convert --to GSE cat accession_list.txt | srake convert --to GSM --format json # Preview conversion without executing srake convert SRP123456 --to GSE --dry-run # Debug mode to see conversion details srake convert SRP123456 --to GSE --debug","examples-3#Examples":"# Get runs for a study srake runs SRP123456 # Get detailed run information srake runs SRX123456 --detailed # Export as JSON srake runs SRP123456 --format json --output runs.json","examples-4#Examples":"# Get samples for a study srake samples SRP123456 # Get detailed sample information srake samples SRP123456 --detailed # Export as CSV srake samples SRX123456 --format csv --output samples.csv","examples-5#Examples":"# Get experiments for a study srake experiments SRP123456 # Get experiments for a sample srake experiments SRS123456 --detailed","examples-6#Examples":"# Get study from an experiment srake studies SRX123456 # Get study from a run with details srake studies SRR123456 --detailed","examples-7#Examples":"# Basic download srake download SRR123456 # Download from AWS with parallel transfers srake download SRR123456 --source aws --threads 4 # Download all runs for a study srake download SRP123456 --type fastq --output ./data/ # Batch download from file srake download --list runs.txt --parallel 4 # Download from stdin (pipe-friendly) echo \"SRR123456\" | srake download --type fastq srake runs SRP123456 | srake download --parallel 4 # High-speed Aspera transfer srake download SRR123456 --aspera # Dry run to preview downloads srake download SRP123456 --dry-run # Non-interactive download (no prompts) srake download SRP123456 --yes # Debug mode for troubleshooting srake download SRR123456 --debug","examples-8#Examples":"# Get metadata for an experiment srake metadata SRX123456 # Get multiple accessions as JSON srake metadata SRX123456 SRX123457 --format json # Select specific fields srake metadata SRR999999 --fields title,platform,strategy","examples-9#Examples":"# Start server on default port srake server # Custom port and host srake server --port 9090 --host 0.0.0.0 # Development mode with debug logging srake server --dev --log-level debug","exit-codes#Exit Codes":"0 - Success 1 - General error 2 - Command line usage error 130 - Interrupted (Ctrl+C)","filtering-flags#Filtering Flags":"--taxon-ids - Filter by taxonomy IDs (comma-separated) --exclude-taxon-ids - Exclude taxonomy IDs --date-from - Start date for filtering --date-to - End date for filtering --organisms - Filter by organism names --platforms - Filter by platforms (ILLUMINA, OXFORD_NANOPORE, etc.) --strategies - Filter by library strategies (RNA-Seq, WGS, etc.) --min-reads - Minimum read count filter --max-reads - Maximum read count filter --stats-only - Only show statistics without inserting data","flags#Flags":"--auto - Auto-select the best file from NCBI --daily - Ingest the latest daily update --monthly - Ingest the latest monthly dataset --file - Ingest specific file (local or NCBI) --list - List available files without ingesting --db - Database path (default: ‚Äú./data/metadata.db‚Äù) --force - Force ingestion even if data exists --no-progress - Disable progress bar","flags-1#Flags":"-o, --organism - Filter by organism --platform - Filter by platform --strategy - Filter by library strategy -l, --limit - Maximum results (default: 100) -f, --format - Output format (table|json|csv|tsv) --output - Save results to file --no-header - Omit header in output","flags-2#Flags":"--to - Target accession type (required) Options: GSE, SRP, SRX, GSM, SRR, SRS, PRJNA, BIOSAMPLE -f, --format - Output format (table|json|yaml|csv|tsv) -o, --output - Save results to file --batch - Read accessions from file --dry-run - Preview conversions without executing","flags-3#Flags":"-d, --detailed - Include detailed information -f, --format - Output format (table|json|yaml|csv|tsv) -o, --output - Save results to file -l, --limit - Limit number of results --fields - Comma-separated list of fields","flags-4#Flags":"-d, --detailed - Include organism and taxonomy information -f, --format - Output format (table|json|yaml|csv|tsv) -o, --output - Save results to file -l, --limit - Limit number of results","flags-5#Flags":"-d, --detailed - Include platform and library information -f, --format - Output format (table|json|yaml|csv|tsv) -o, --output - Save results to file -l, --limit - Limit number of results","flags-6#Flags":"-d, --detailed - Include abstract and full metadata -f, --format - Output format (table|json|yaml|csv|tsv) -o, --output - Save results to file","flags-7#Flags":"-s, --source - Download source (auto|ftp|aws|gcp|ncbi) -t, --type - File type (sra|fastq|fasta) -o, --output - Output directory (default: ‚Äú./‚Äù) --threads - Download threads per file (default: 1) -p, --parallel - Parallel downloads (default: 1) --aspera - Use Aspera for high-speed transfer -l, --list - File containing accessions --retry - Number of retry attempts (default: 3) --validate - Validate downloaded files (default: true) --dry-run - Show what would be downloaded","flags-8#Flags":"-f, --format - Output format (table|json|yaml) --fields - Comma-separated list of fields --expand - Expand nested structures","flags-9#Flags":"-p, --port - Port to listen on (default: 8080) --host - Host to bind to (default: localhost) --db - Database path --log-level - Log level (debug|info|warn|error) --dev - Enable development mode","global-flags#Global Flags":"These flags are available for all commands:\n--no-color - Disable colored output -v, --verbose - Enable verbose output -q, --quiet - Suppress non-error output -y, --yes - Assume yes to all prompts (non-interactive mode) --debug - Enable debug output for troubleshooting --help - Show help for any command","output-formats#Output Formats":"Most commands support multiple output formats:\ntable (default) - Human-readable table with colors json - JSON format for programmatic use yaml - YAML format csv - Comma-separated values tsv - Tab-separated values xml - XML format (convert command only)","srake-convert#\u003ccode\u003esrake convert\u003c/code\u003e":"Convert between different accession types (SRA, GEO, BioProject, BioSample).\nsrake convert [ ...] [flags]","srake-db#\u003ccode\u003esrake db\u003c/code\u003e":"Database management commands.\nsrake db [flags]","srake-download#\u003ccode\u003esrake download\u003c/code\u003e":"Download SRA data files from multiple sources.\nsrake download [ ...] [flags]","srake-experiments#\u003ccode\u003esrake experiments\u003c/code\u003e":"Get all experiments for a study or sample.\nsrake experiments [flags]","srake-ingest#\u003ccode\u003esrake ingest\u003c/code\u003e":"Ingest SRA metadata from NCBI or local archives.\nsrake ingest [flags]","srake-metadata#\u003ccode\u003esrake metadata\u003c/code\u003e":"Get detailed metadata for specific accessions.\nsrake metadata [accessions...] [flags]","srake-runs#\u003ccode\u003esrake runs\u003c/code\u003e":"Get all runs for a study, experiment, or sample.\nsrake runs [flags]","srake-samples#\u003ccode\u003esrake samples\u003c/code\u003e":"Get all samples for a study or experiment.\nsrake samples [flags]","srake-search#\u003ccode\u003esrake search\u003c/code\u003e":"Search SRA metadata for experiments matching your query.\nsrake search [flags]","srake-server#\u003ccode\u003esrake server\u003c/code\u003e":"Start the API server for programmatic access.\nsrake server [flags]","srake-studies#\u003ccode\u003esrake studies\u003c/code\u003e":"Get study information for any SRA accession.\nsrake studies [flags]","subcommands#Subcommands":"info - Show database statistics and information","supported-conversions#Supported Conversions":"From To Description SRP GSE, SRX, SRR, SRS, PRJNA Study to related accessions SRX GSM, SRP, SRR, SRS Experiment to related accessions SRR SRX, SRP, GSM Run to parent accessions SRS SRX, GSM, BIOSAMPLE Sample to related accessions GSE SRP, GSM GEO Series to SRA/samples GSM SRX, SRR, GSE GEO Sample to SRA/series PRJNA SRP BioProject to SRA Project SAMN SRS BioSample to SRA Sample"},"title":"CLI Reference"}}